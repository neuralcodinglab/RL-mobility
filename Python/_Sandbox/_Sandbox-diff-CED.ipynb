{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm_notebook\n",
    "# from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import cv2\n",
    "# import imgproc\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import os \n",
    "\n",
    "# local files\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "from model import Transition\n",
    "import training\n",
    "import imgproc\n",
    "\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_5\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_10\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_18\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_26\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_34\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_42\n",
      "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_50\n",
      "exp1-aug-11_comp_complex-seed_0-mode_camera-vision\n",
      "exp1-aug-11_comp_complex-seed_0-mode_no-vision\n",
      "exp1-aug-11_comp_complex-seed_1-mode_camera-vision\n",
      "exp1-aug-11_comp_complex-seed_1-mode_no-vision\n",
      "exp1-aug-11_comp_complex-seed_2-mode_camera-vision\n",
      "exp1-aug-11_comp_complex-seed_2-mode_no-vision\n",
      "exp1-aug-11_comp_plain-seed_0-mode_camera-vision\n",
      "exp1-aug-11_comp_plain-seed_0-mode_no-vision\n",
      "exp1-aug-11_comp_plain-seed_1-mode_camera-vision\n",
      "exp1-aug-11_comp_plain-seed_1-mode_no-vision\n",
      "exp1-aug-11_comp_plain-seed_2-mode_camera-vision\n",
      "exp1-aug-11_comp_plain-seed_2-mode_no-vision\n"
     ]
    }
   ],
   "source": [
    "train_specs = pd.read_csv(\"../Experiments/Out/Exp1_Aug_11/_specs.csv\").set_index('model_name')\n",
    "\n",
    "for i, cfg in train_specs.iterrows():\n",
    "    print(cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='phosphene_resolution', ylabel='cumulative_reward'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(data=train_specs, x='phosphene_resolution', y='cumulative_reward', hue='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_5     False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_10    False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_18    False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_26    False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_34    False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_42    False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_edge-detection-phos_50    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_5     False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_10    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_18    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_26    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_34    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_42    False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_edge-detection-phos_50    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_5     False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_10    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_18    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_26    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_34    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_42    False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_edge-detection-phos_50    False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_5       False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_10      False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_18      False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_26      False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_34      False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_42      False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_edge-detection-phos_50      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_5       False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_10      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_18      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_26      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_34      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_42      False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_edge-detection-phos_50      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_5       False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_10      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_18      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_26      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_34      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_42      False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_edge-detection-phos_50      False\n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision              True\n",
       "exp1-aug-11_comp_complex-seed_0-mode_no-vision                 False\n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision              True\n",
       "exp1-aug-11_comp_complex-seed_1-mode_no-vision                 False\n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision              True\n",
       "exp1-aug-11_comp_complex-seed_2-mode_no-vision                 False\n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision                True\n",
       "exp1-aug-11_comp_plain-seed_0-mode_no-vision                   False\n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision                True\n",
       "exp1-aug-11_comp_plain-seed_1-mode_no-vision                   False\n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision                True\n",
       "exp1-aug-11_comp_plain-seed_2-mode_no-vision                   False\n",
       "Name: mode, dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_specs['mode']=='camera-vision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>complexity</th>\n",
       "      <th>seed</th>\n",
       "      <th>mode</th>\n",
       "      <th>phosphene_resolution</th>\n",
       "      <th>name</th>\n",
       "      <th>descr</th>\n",
       "      <th>fwd_reward</th>\n",
       "      <th>side_reward</th>\n",
       "      <th>box_reward</th>\n",
       "      <th>...</th>\n",
       "      <th>replay_start_size</th>\n",
       "      <th>target_update</th>\n",
       "      <th>memory_capacity</th>\n",
       "      <th>dist_feedback</th>\n",
       "      <th>edge_threshold</th>\n",
       "      <th>wall_collisions</th>\n",
       "      <th>box_collisions</th>\n",
       "      <th>endless_loops</th>\n",
       "      <th>step_count</th>\n",
       "      <th>cumulative_reward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_complex-seed_0-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>complex</td>\n",
       "      <td>0</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_complex-seed_1-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>complex</td>\n",
       "      <td>1</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_complex-seed_2-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_plain-seed_0-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>plain</td>\n",
       "      <td>0</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_plain-seed_1-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>plain</td>\n",
       "      <td>1</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp1-aug-11_comp_plain-seed_2-mode_camera-vision</th>\n",
       "      <td>finished</td>\n",
       "      <td>plain</td>\n",
       "      <td>2</td>\n",
       "      <td>camera-vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exp1-Aug-11</td>\n",
       "      <td>Different phosphene resolutions</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>50000</td>\n",
       "      <td>36000</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      status complexity  seed  \\\n",
       "model_name                                                                      \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision  finished    complex     0   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision  finished    complex     1   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision  finished    complex     2   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision    finished      plain     0   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision    finished      plain     1   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision    finished      plain     2   \n",
       "\n",
       "                                                             mode  \\\n",
       "model_name                                                          \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision  camera-vision   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision  camera-vision   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision  camera-vision   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision    camera-vision   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision    camera-vision   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision    camera-vision   \n",
       "\n",
       "                                                    phosphene_resolution  \\\n",
       "model_name                                                                 \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision                   NaN   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision                   NaN   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision                   NaN   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision                     NaN   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision                     NaN   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision                     NaN   \n",
       "\n",
       "                                                           name  \\\n",
       "model_name                                                        \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision  Exp1-Aug-11   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision  Exp1-Aug-11   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision  Exp1-Aug-11   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision    Exp1-Aug-11   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision    Exp1-Aug-11   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision    Exp1-Aug-11   \n",
       "\n",
       "                                                                              descr  \\\n",
       "model_name                                                                            \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision  Different phosphene resolutions   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision  Different phosphene resolutions   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision  Different phosphene resolutions   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision    Different phosphene resolutions   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision    Different phosphene resolutions   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision    Different phosphene resolutions   \n",
       "\n",
       "                                                    fwd_reward  side_reward  \\\n",
       "model_name                                                                    \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision          10           -1   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision          10           -1   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision          10           -1   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision            10           -1   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision            10           -1   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision            10           -1   \n",
       "\n",
       "                                                    box_reward  ...  \\\n",
       "model_name                                                      ...   \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision         -20  ...   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision         -20  ...   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision         -20  ...   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision           -20  ...   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision           -20  ...   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision           -20  ...   \n",
       "\n",
       "                                                    replay_start_size  \\\n",
       "model_name                                                              \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision               1500   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision               1500   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision               1500   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision                 1500   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision                 1500   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision                 1500   \n",
       "\n",
       "                                                    target_update  \\\n",
       "model_name                                                          \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision          50000   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision          50000   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision          50000   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision            50000   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision            50000   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision            50000   \n",
       "\n",
       "                                                    memory_capacity  \\\n",
       "model_name                                                            \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision            36000   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision            36000   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision            36000   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision              36000   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision              36000   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision              36000   \n",
       "\n",
       "                                                    dist_feedback  \\\n",
       "model_name                                                          \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision          False   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision          False   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision          False   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision            False   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision            False   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision            False   \n",
       "\n",
       "                                                    edge_threshold  \\\n",
       "model_name                                                           \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision             120   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision             120   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision             120   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision               120   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision               120   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision               120   \n",
       "\n",
       "                                                   wall_collisions  \\\n",
       "model_name                                                           \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision             NaN   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision             NaN   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision             NaN   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision               NaN   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision               NaN   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision               NaN   \n",
       "\n",
       "                                                   box_collisions  \\\n",
       "model_name                                                          \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision            NaN   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision            NaN   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision            NaN   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision              NaN   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision              NaN   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision              NaN   \n",
       "\n",
       "                                                   endless_loops  step_count  \\\n",
       "model_name                                                                     \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision           NaN         NaN   \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision           NaN         NaN   \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision           NaN         NaN   \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision             NaN         NaN   \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision             NaN         NaN   \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision             NaN         NaN   \n",
       "\n",
       "                                                    cumulative_reward  \n",
       "model_name                                                             \n",
       "exp1-aug-11_comp_complex-seed_0-mode_camera-vision                NaN  \n",
       "exp1-aug-11_comp_complex-seed_1-mode_camera-vision                NaN  \n",
       "exp1-aug-11_comp_complex-seed_2-mode_camera-vision                NaN  \n",
       "exp1-aug-11_comp_plain-seed_0-mode_camera-vision                  NaN  \n",
       "exp1-aug-11_comp_plain-seed_1-mode_camera-vision                  NaN  \n",
       "exp1-aug-11_comp_plain-seed_2-mode_camera-vision                  NaN  \n",
       "\n",
       "[6 rows x 49 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_specs.loc[train_specs['mode']=='camera-vision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# local files\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "import imgproc\n",
    "from model import Transition\n",
    "import testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEnvironment:\n",
    "    def __init__(self, ip = \"127.0.0.1\", port = 13000, size = 128, channels=16, *args,**kwargs):\n",
    "        self.ip         = ip\n",
    "        self.port       = port\n",
    "        self.size       = size\n",
    "        self.channels   = channels\n",
    "\n",
    "    def reset(self, kind=0):\n",
    "         # 0: plain 1: complex 2: plain_test 3: complex_test\n",
    "        self._send(1, kind)\n",
    "        return self._receive()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._send(2, action)\n",
    "        return self._receive()\n",
    "\n",
    "    def setRandomSeed(self, action):\n",
    "        self._send(3, action)\n",
    "        return self._receive()\n",
    "\n",
    "    def _receive(self):\n",
    "        end    = np.random.choice([0,1,2,3], p=[0.7,0.1,0.1,0.1])\n",
    "        reward = 99 #int(255*np.random.rand())\n",
    "        state  = {'colors': (np.random.rand(self.size,self.size,3)*255).astype('uint8')}\n",
    "        return end, reward, state\n",
    "\n",
    "    def _send(self, action, command):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adaptive edge threshold\n"
     ]
    }
   ],
   "source": [
    "savedir = \"../Experiments/Out/Exp3_Aug_11/\"\n",
    "current_model = cfg.name\n",
    "cfg['device'] = 'cpu'\n",
    "cfg['stack_size'] = 1\n",
    "cfg['in_channels'] = cfg['stack_size']\n",
    "cfg['batch_size'] = 3\n",
    "cfg['optimizations_per_step'] = 1\n",
    "# cfg['lr_dqn'] = 0.9\n",
    "cfg['edge_threshold'] = 120\n",
    "\n",
    "# Additional training settings (inferred from specified settings)\n",
    "cfg['training_condition'] = {'plain': 0, 'complex': 1}[cfg['complexity']]\n",
    "cfg['model_path']         = os.path.join(savedir,'{}.pth'.format(current_model)) # Save path for model\n",
    "cfg['logfile']            = os.path.join(savedir,'{}_train_stats.csv'.format(current_model)) # To save the training stats\n",
    "\n",
    "#         # Write train_specs to csvfile\n",
    "#         if train_specs.loc[current_model, 'status'] == 'finished':\n",
    "#             print('skipping.. already finished in previous training: {}'.format(current_model))\n",
    "#             continue\n",
    "#         train_specs.loc[current_model, 'status'] = 'training'\n",
    "#         train_specs.to_csv(specs_file)\n",
    "#         print(train_specs)\n",
    "\n",
    "# Initialize model components\n",
    "torch.manual_seed(cfg['seed'])\n",
    "# agent = model.DoubleDQNAgent(**cfg)\n",
    "agent = model.AdaptiveAgent(**cfg)\n",
    "# img_processing = imgproc.ImageProcessor(**cfg)\n",
    "if cfg['adaptive_threshold']:\n",
    "    print(\"Using adaptive edge threshold\") # TODO: this print statement can be removed\n",
    "    optimizer = optim.Adam([*agent.canny_layer.parameters(), *agent.policy_net.parameters()], lr = cfg['lr_dqn'])\n",
    "else:\n",
    "    optimizer = optim.Adam(agent.policy_net.parameters(), lr = cfg['lr_dqn'])\n",
    "\n",
    "environment = DummyEnvironment(**cfg)\n",
    "    \n",
    "# environment =  pyClient.Environment(**cfg) #if not environment_connected else environment # Only initialized on first run\n",
    "\n",
    "#         # # Training\n",
    "#         assert environment.client is not None, \"Error: could not connect to env. Make sure to start Unity server first!\"\n",
    "#         environment_connected = True\n",
    "#         train(agent, environment, img_processing, optimizer, cfg)\n",
    "\n",
    "#         # Write specs to training file\n",
    "#         train_specs.loc[current_model, 'status'] = 'finished'\n",
    "#         train_specs.to_csv(specs_file)\n",
    "#         print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, step count: 40 wall_collisions: 4, box_collisions: 8, endless_loops: 7, total_reward: 2574.0, threshold: 0.4706\n",
      "model improved! Saving to ../Experiments/Out/Exp3_Aug_11/exp3-aug11_comp_plain-seed_2-mode_edge-detection-phos_34-edge_1000.pth\n",
      "Target net updated!\n",
      "episode 1, step count: 46 wall_collisions: 2, box_collisions: 3, endless_loops: 9, total_reward: 4158.0, threshold: 0.4706\n",
      "model improved! Saving to ../Experiments/Out/Exp3_Aug_11/exp3-aug11_comp_plain-seed_2-mode_edge-detection-phos_34-edge_1000.pth\n",
      "episode 3, step count: 104 wall_collisions: 11, box_collisions: 5, endless_loops: 0, total_reward: 7029.0, threshold: 0.4601\n",
      "model improved! Saving to ../Experiments/Out/Exp3_Aug_11/exp3-aug11_comp_plain-seed_2-mode_edge-detection-phos_34-edge_1000.pth\n",
      "episode 13, step count: 151 wall_collisions: 12, box_collisions: 19, endless_loops: 0, total_reward: 2574.0, threshold: 0.4012\n",
      "episode 25, step count: 38 wall_collisions: 3, box_collisions: 1, endless_loops: 6, total_reward: 3861.0, threshold: 0.3815\n",
      "episode 27, step count: 39 wall_collisions: 3, box_collisions: 5, endless_loops: 1, total_reward: 2970.0, threshold: 0.3772\n",
      "episode 50, step count: 39 wall_collisions: 5, box_collisions: 5, endless_loops: 7, total_reward: 3168.0, threshold: 0.3673\n",
      "episode 75, step count: 102 wall_collisions: 15, box_collisions: 11, endless_loops: 0, total_reward: 3267.0, threshold: 0.3621\n",
      "episode 100, step count: 39 wall_collisions: 4, box_collisions: 4, endless_loops: 7, total_reward: 3168.0, threshold: 0.3525\n",
      "episode 125, step count: 28 wall_collisions: 2, box_collisions: 4, endless_loops: 5, total_reward: 2376.0, threshold: 0.3426\n",
      "episode 150, step count: 46 wall_collisions: 8, box_collisions: 5, endless_loops: 5, total_reward: 3564.0, threshold: 0.3280\n",
      "episode 175, step count: 31 wall_collisions: 5, box_collisions: 1, endless_loops: 5, total_reward: 3267.0, threshold: 0.3134\n",
      "episode 200, step count: 64 wall_collisions: 4, box_collisions: 7, endless_loops: 2, total_reward: 4059.0, threshold: 0.3042\n",
      "episode 225, step count: 39 wall_collisions: 4, box_collisions: 3, endless_loops: 7, total_reward: 3663.0, threshold: 0.2951\n",
      "episode 250, step count: 55 wall_collisions: 3, box_collisions: 9, endless_loops: 5, total_reward: 2178.0, threshold: 0.2822\n",
      "episode 275, step count: 58 wall_collisions: 4, box_collisions: 8, endless_loops: 1, total_reward: 4158.0, threshold: 0.2677\n",
      "episode 282, step count: 31 wall_collisions: 5, box_collisions: 6, endless_loops: 5, total_reward: 2376.0, threshold: 0.2637\n",
      "episode 300, step count: 97 wall_collisions: 12, box_collisions: 13, endless_loops: 0, total_reward: 3960.0, threshold: 0.2555\n",
      "episode 303, step count: 23 wall_collisions: 3, box_collisions: 5, endless_loops: 2, total_reward: 2079.0, threshold: 0.2539\n",
      "episode 325, step count: 36 wall_collisions: 3, box_collisions: 7, endless_loops: 5, total_reward: 2871.0, threshold: 0.2391\n",
      "episode 350, step count: 50 wall_collisions: 9, box_collisions: 4, endless_loops: 4, total_reward: 4059.0, threshold: 0.2300\n",
      "episode 375, step count: 143 wall_collisions: 10, box_collisions: 10, endless_loops: 0, total_reward: 8019.0, threshold: 0.2143\n",
      "model improved! Saving to ../Experiments/Out/Exp3_Aug_11/exp3-aug11_comp_plain-seed_2-mode_edge-detection-phos_34-edge_1000.pth\n",
      "episode 400, step count: 39 wall_collisions: 2, box_collisions: 7, endless_loops: 6, total_reward: 2277.0, threshold: 0.1924\n",
      "episode 425, step count: 72 wall_collisions: 10, box_collisions: 7, endless_loops: 3, total_reward: 4059.0, threshold: 0.1793\n",
      "episode 450, step count: 75 wall_collisions: 7, box_collisions: 12, endless_loops: 4, total_reward: 3861.0, threshold: 0.1590\n",
      "episode 462, step count: 33 wall_collisions: 3, box_collisions: 8, endless_loops: 6, total_reward: 2178.0, threshold: 0.1514\n",
      "episode 475, step count: 60 wall_collisions: 14, box_collisions: 5, endless_loops: 7, total_reward: 3663.0, threshold: 0.1465\n",
      "episode 500, step count: 36 wall_collisions: 7, box_collisions: 5, endless_loops: 2, total_reward: 2574.0, threshold: 0.1278\n",
      "episode 525, step count: 31 wall_collisions: 5, box_collisions: 2, endless_loops: 4, total_reward: 3168.0, threshold: 0.1110\n",
      "episode 550, step count: 39 wall_collisions: 5, box_collisions: 4, endless_loops: 7, total_reward: 2871.0, threshold: 0.1006\n",
      "episode 575, step count: 112 wall_collisions: 18, box_collisions: 7, endless_loops: 0, total_reward: 7128.0, threshold: 0.0910\n",
      "episode 600, step count: 116 wall_collisions: 15, box_collisions: 6, endless_loops: 1, total_reward: 5247.0, threshold: 0.0729\n",
      "episode 625, step count: 87 wall_collisions: 17, box_collisions: 6, endless_loops: 4, total_reward: 4059.0, threshold: 0.0614\n",
      "episode 650, step count: 36 wall_collisions: 5, box_collisions: 4, endless_loops: 2, total_reward: 2871.0, threshold: 0.0444\n",
      "episode 675, step count: 41 wall_collisions: 6, box_collisions: 3, endless_loops: 3, total_reward: 4059.0, threshold: 0.0232\n",
      "episode 700, step count: 81 wall_collisions: 9, box_collisions: 7, endless_loops: 1, total_reward: 6930.0, threshold: 0.0154\n",
      "episode 725, step count: 91 wall_collisions: 11, box_collisions: 13, endless_loops: 0, total_reward: 3861.0, threshold: -0.0043\n",
      "episode 750, step count: 96 wall_collisions: 10, box_collisions: 4, endless_loops: 0, total_reward: 7425.0, threshold: -0.0044\n",
      "episode 775, step count: 40 wall_collisions: 4, box_collisions: 5, endless_loops: 7, total_reward: 3366.0, threshold: -0.0044\n",
      "episode 800, step count: 67 wall_collisions: 4, box_collisions: 11, endless_loops: 0, total_reward: 3366.0, threshold: -0.0044\n",
      "episode 825, step count: 44 wall_collisions: 5, box_collisions: 4, endless_loops: 8, total_reward: 3861.0, threshold: -0.0044\n",
      "episode 850, step count: 32 wall_collisions: 6, box_collisions: 3, endless_loops: 5, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 875, step count: 97 wall_collisions: 12, box_collisions: 10, endless_loops: 0, total_reward: 2772.0, threshold: -0.0044\n",
      "episode 900, step count: 37 wall_collisions: 4, box_collisions: 5, endless_loops: 6, total_reward: 2475.0, threshold: -0.0044\n",
      "episode 925, step count: 41 wall_collisions: 8, box_collisions: 5, endless_loops: 8, total_reward: 2772.0, threshold: -0.0044\n",
      "episode 950, step count: 36 wall_collisions: 5, box_collisions: 2, endless_loops: 6, total_reward: 3069.0, threshold: -0.0044\n",
      "episode 975, step count: 44 wall_collisions: 7, box_collisions: 2, endless_loops: 7, total_reward: 4158.0, threshold: -0.0044\n",
      "episode 1000, step count: 35 wall_collisions: 4, box_collisions: 5, endless_loops: 5, total_reward: 3366.0, threshold: -0.0044\n",
      "episode 1025, step count: 41 wall_collisions: 3, box_collisions: 7, endless_loops: 8, total_reward: 2871.0, threshold: -0.0044\n",
      "episode 1050, step count: 41 wall_collisions: 8, box_collisions: 4, endless_loops: 6, total_reward: 3564.0, threshold: -0.0044\n",
      "episode 1075, step count: 30 wall_collisions: 5, box_collisions: 4, endless_loops: 4, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 1100, step count: 33 wall_collisions: 8, box_collisions: 1, endless_loops: 3, total_reward: 3663.0, threshold: -0.0044\n",
      "episode 1125, step count: 119 wall_collisions: 14, box_collisions: 19, endless_loops: 0, total_reward: 3069.0, threshold: -0.0044\n",
      "episode 1150, step count: 46 wall_collisions: 10, box_collisions: 1, endless_loops: 8, total_reward: 4653.0, threshold: -0.0044\n",
      "episode 1175, step count: 89 wall_collisions: 8, box_collisions: 13, endless_loops: 0, total_reward: 5148.0, threshold: -0.0044\n",
      "episode 1200, step count: 34 wall_collisions: 2, box_collisions: 8, endless_loops: 6, total_reward: 2376.0, threshold: -0.0044\n",
      "episode 1225, step count: 35 wall_collisions: 4, box_collisions: 5, endless_loops: 6, total_reward: 2673.0, threshold: -0.0044\n",
      "episode 1250, step count: 34 wall_collisions: 3, box_collisions: 5, endless_loops: 6, total_reward: 2277.0, threshold: -0.0044\n",
      "episode 1275, step count: 36 wall_collisions: 6, box_collisions: 5, endless_loops: 5, total_reward: 3465.0, threshold: -0.0044\n",
      "episode 1300, step count: 24 wall_collisions: 5, box_collisions: 1, endless_loops: 4, total_reward: 2574.0, threshold: -0.0044\n",
      "episode 1325, step count: 43 wall_collisions: 7, box_collisions: 4, endless_loops: 8, total_reward: 3168.0, threshold: -0.0044\n",
      "episode 1350, step count: 129 wall_collisions: 19, box_collisions: 13, endless_loops: 0, total_reward: 6336.0, threshold: -0.0044\n",
      "episode 1375, step count: 120 wall_collisions: 13, box_collisions: 11, endless_loops: 0, total_reward: 4554.0, threshold: -0.0044\n",
      "episode 1400, step count: 33 wall_collisions: 4, box_collisions: 7, endless_loops: 4, total_reward: 1584.0, threshold: -0.0044\n",
      "episode 1425, step count: 46 wall_collisions: 7, box_collisions: 3, endless_loops: 8, total_reward: 4158.0, threshold: -0.0044\n",
      "episode 1450, step count: 36 wall_collisions: 5, box_collisions: 5, endless_loops: 6, total_reward: 2574.0, threshold: -0.0044\n",
      "episode 1475, step count: 86 wall_collisions: 12, box_collisions: 13, endless_loops: 0, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 1500, step count: 40 wall_collisions: 3, box_collisions: 4, endless_loops: 0, total_reward: 2772.0, threshold: -0.0044\n",
      "episode 1525, step count: 55 wall_collisions: 6, box_collisions: 3, endless_loops: 0, total_reward: 5445.0, threshold: -0.0044\n",
      "episode 1550, step count: 36 wall_collisions: 3, box_collisions: 5, endless_loops: 5, total_reward: 2673.0, threshold: -0.0044\n",
      "episode 1575, step count: 80 wall_collisions: 6, box_collisions: 9, endless_loops: 0, total_reward: 3564.0, threshold: -0.0044\n",
      "episode 1600, step count: 72 wall_collisions: 4, box_collisions: 6, endless_loops: 0, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 1625, step count: 48 wall_collisions: 6, box_collisions: 1, endless_loops: 9, total_reward: 4752.0, threshold: -0.0044\n",
      "episode 1650, step count: 39 wall_collisions: 8, box_collisions: 2, endless_loops: 7, total_reward: 3663.0, threshold: -0.0044\n",
      "episode 1675, step count: 34 wall_collisions: 9, box_collisions: 0, endless_loops: 5, total_reward: 3861.0, threshold: -0.0044\n",
      "episode 1700, step count: 100 wall_collisions: 11, box_collisions: 6, endless_loops: 0, total_reward: 7722.0, threshold: -0.0044\n",
      "episode 1725, step count: 28 wall_collisions: 3, box_collisions: 3, endless_loops: 4, total_reward: 2871.0, threshold: -0.0044\n",
      "episode 1750, step count: 40 wall_collisions: 6, box_collisions: 4, endless_loops: 4, total_reward: 3960.0, threshold: -0.0044\n",
      "episode 1775, step count: 41 wall_collisions: 3, box_collisions: 7, endless_loops: 8, total_reward: 3069.0, threshold: -0.0044\n",
      "episode 1800, step count: 21 wall_collisions: 3, box_collisions: 4, endless_loops: 3, total_reward: 1485.0, threshold: -0.0044\n",
      "episode 1825, step count: 32 wall_collisions: 6, box_collisions: 3, endless_loops: 3, total_reward: 3663.0, threshold: -0.0044\n",
      "episode 1850, step count: 29 wall_collisions: 2, box_collisions: 1, endless_loops: 4, total_reward: 3267.0, threshold: -0.0044\n",
      "episode 1875, step count: 31 wall_collisions: 4, box_collisions: 3, endless_loops: 4, total_reward: 3267.0, threshold: -0.0044\n",
      "episode 1900, step count: 39 wall_collisions: 6, box_collisions: 6, endless_loops: 0, total_reward: 2178.0, threshold: -0.0044\n",
      "episode 1925, step count: 82 wall_collisions: 11, box_collisions: 10, endless_loops: 0, total_reward: 4950.0, threshold: -0.0044\n",
      "episode 1950, step count: 41 wall_collisions: 6, box_collisions: 7, endless_loops: 7, total_reward: 2475.0, threshold: -0.0044\n",
      "episode 1975, step count: 48 wall_collisions: 7, box_collisions: 1, endless_loops: 0, total_reward: 5544.0, threshold: -0.0044\n",
      "episode 2000, step count: 33 wall_collisions: 2, box_collisions: 7, endless_loops: 5, total_reward: 2277.0, threshold: -0.0044\n",
      "episode 2025, step count: 42 wall_collisions: 4, box_collisions: 5, endless_loops: 8, total_reward: 3168.0, threshold: -0.0044\n",
      "episode 2050, step count: 40 wall_collisions: 5, box_collisions: 8, endless_loops: 7, total_reward: 2871.0, threshold: -0.0044\n",
      "episode 2075, step count: 34 wall_collisions: 6, box_collisions: 4, endless_loops: 5, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 2100, step count: 105 wall_collisions: 7, box_collisions: 13, endless_loops: 0, total_reward: 2772.0, threshold: -0.0044\n",
      "episode 2125, step count: 37 wall_collisions: 6, box_collisions: 6, endless_loops: 5, total_reward: 2574.0, threshold: -0.0044\n",
      "episode 2150, step count: 21 wall_collisions: 4, box_collisions: 3, endless_loops: 3, total_reward: 2079.0, threshold: -0.0044\n",
      "episode 2175, step count: 35 wall_collisions: 3, box_collisions: 5, endless_loops: 5, total_reward: 2970.0, threshold: -0.0044\n",
      "episode 2200, step count: 117 wall_collisions: 9, box_collisions: 17, endless_loops: 0, total_reward: 5445.0, threshold: -0.0044\n",
      "episode 2225, step count: 29 wall_collisions: 3, box_collisions: 4, endless_loops: 4, total_reward: 2475.0, threshold: -0.0044\n",
      "episode 2250, step count: 43 wall_collisions: 5, box_collisions: 2, endless_loops: 7, total_reward: 4059.0, threshold: -0.0044\n",
      "episode 2275, step count: 27 wall_collisions: 3, box_collisions: 6, endless_loops: 3, total_reward: 2475.0, threshold: -0.0044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-23f88117d209>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtraining_adaptive_threshold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\training_adaptive_threshold.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, environment, img_processing, optimizer_dqn, cfg, optimizer_canny, scheduler_canny)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizations_per_step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mstate_action_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_state_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                     \u001b[1;31m# Compute Huber loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnon_final_next_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mnon_final_next_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Concatenate actions and rewards into batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\model.py\u001b[0m in \u001b[0;36mprocess_observation\u001b[1;34m(self, observation, validation)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanny_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanny_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanny_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mmagnitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_maxima\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscikit_canny\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_canny\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagnitude\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlocal_maxima\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\scikit_canny.py\u001b[0m in \u001b[0;36m_canny\u001b[1;34m(image, sigma, low_threshold, high_threshold, mask, use_quantiles, mode, cval)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;31m# Non-maximum suppression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m     \u001b[0mlocal_maxima\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_local_maxima\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misobel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjsobel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meroded_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;31m# Double thresholding and edge traking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\scikit_canny.py\u001b[0m in \u001b[0;36m_get_local_maxima\u001b[1;34m(isobel, jsobel, magnitude, eroded_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         local_maxima)\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;31m# ----- 90 to 135 degrees ------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;31m# Mix anti-diagonal and vertical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NESTOR\\RL-mobility\\Python\\scikit_canny.py\u001b[0m in \u001b[0;36m_set_local_maxima\u001b[1;34m(magnitude, pts, w_num, w_denum, row_slices, col_slices, out)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mw_denum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[0mc_plus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training_adaptive_threshold import train, validation_loop\n",
    "\n",
    "train(agent, environment, None, optimizer, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.policy_net(state_batch).gather(1, action_batch).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(12.3969, requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.canny_layer.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = agent.memory.sample(agent.batch_size)\n",
    "batch = Transition(*zip(*transitions))\n",
    "\n",
    "# Concatenate (next) states into batch and process with canny and simulator\n",
    "if type(batch.state[0]) == np.ndarray:\n",
    "    state_batch = np.concatenate(batch.state)\n",
    "elif type(batch.state[0]) == torch.Tensor:\n",
    "    state_batch = torch.cat(batch.state) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad before: 0.0\n",
      "grad after: 0.0008726119995117188\n"
     ]
    }
   ],
   "source": [
    "y = agent.process_observation(state_batch)\n",
    "pred = agent.policy_net(y)\n",
    "loss = pred.mean()\n",
    "\n",
    "# state_action_values, expected_state_action_values = agent.forward()\n",
    "# loss = 1e100 * state_action_values.mean()\n",
    "\n",
    "grad = agent.canny_layer.threshold.grad\n",
    "agent.canny_layer.zero_grad()\n",
    "print(f\"grad before: {grad}\")\n",
    "loss.backward()\n",
    "grad = agent.canny_layer.threshold.grad\n",
    "print(f\"grad after: {grad}\")\n",
    "\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,1,.01, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaap\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    optimiz\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearLR in module torch.optim.lr_scheduler:\n",
      "\n",
      "class LinearLR(_LRScheduler)\n",
      " |  Decays the learning rate of each parameter group by linearly changing small\n",
      " |  multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.\n",
      " |  Notice that such decay can happen simultaneously with other changes to the learning rate\n",
      " |  from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
      " |  \n",
      " |  Args:\n",
      " |      optimizer (Optimizer): Wrapped optimizer.\n",
      " |      start_factor (float): The number we multiply learning rate in the first epoch.\n",
      " |          The multiplication factor changes towards end_factor in the following epochs.\n",
      " |          Default: 1./3.\n",
      " |      end_factor (float): The number we multiply learning rate at the end of linear changing\n",
      " |          process. Default: 1.0.\n",
      " |      total_iters (int): The number of iterations that multiplicative factor reaches to 1.\n",
      " |          Default: 5.\n",
      " |      last_epoch (int): The index of the last epoch. Default: -1.\n",
      " |      verbose (bool): If ``True``, prints a message to stdout for\n",
      " |          each update. Default: ``False``.\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      " |      >>> # lr = 0.025    if epoch == 0\n",
      " |      >>> # lr = 0.03125  if epoch == 1\n",
      " |      >>> # lr = 0.0375   if epoch == 2\n",
      " |      >>> # lr = 0.04375  if epoch == 3\n",
      " |      >>> # lr = 0.005    if epoch >= 4\n",
      " |      >>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)\n",
      " |      >>> for epoch in range(100):\n",
      " |      >>>     train(...)\n",
      " |      >>>     validate(...)\n",
      " |      >>>     scheduler.step()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearLR\n",
      " |      _LRScheduler\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, optimizer, start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_lr(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _LRScheduler:\n",
      " |  \n",
      " |  get_last_lr(self)\n",
      " |      Return last computed learning rate by current scheduler.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the schedulers state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): scheduler state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      " |      Display the current learning rate.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the scheduler as a :class:`dict`.\n",
      " |      \n",
      " |      It contains an entry for every variable in self.__dict__ which\n",
      " |      is not the optimizer.\n",
      " |  \n",
      " |  step(self, epoch=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _LRScheduler:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.lr_scheduler.LinearLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChainedScheduler',\n",
       " 'ConstantLR',\n",
       " 'CosineAnnealingLR',\n",
       " 'CosineAnnealingWarmRestarts',\n",
       " 'Counter',\n",
       " 'CyclicLR',\n",
       " 'EPOCH_DEPRECATION_WARNING',\n",
       " 'ExponentialLR',\n",
       " 'LambdaLR',\n",
       " 'LinearLR',\n",
       " 'MultiStepLR',\n",
       " 'MultiplicativeLR',\n",
       " 'OneCycleLR',\n",
       " 'Optimizer',\n",
       " 'ReduceLROnPlateau',\n",
       " 'SequentialLR',\n",
       " 'StepLR',\n",
       " '_LRScheduler',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'bisect_right',\n",
       " 'inf',\n",
       " 'math',\n",
       " 'types',\n",
       " 'warnings',\n",
       " 'weakref',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.optim.lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.7456e+04, 9.0400e+02, 4.4100e+02, 1.1000e+01, 5.9000e+01,\n",
       "        5.3000e+01, 7.2000e+01, 4.5000e+01, 5.7000e+01, 5.4000e+01]),\n",
       " array([0.        , 0.06367389, 0.12734778, 0.19102167, 0.25469556,\n",
       "        0.31836945, 0.38204333, 0.44571722, 0.5093911 , 0.573065  ,\n",
       "        0.6367389 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y.flatten().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values, expected_state_action_values = agent.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([118.7073, 118.4582, 121.0643], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_adaptive_threshold import validation_loop\n",
    "\n",
    "# For reproducability, reset the RNG seed\n",
    "torch.manual_seed(cfg['seed'])\n",
    "\n",
    "# Write header to logfile\n",
    "with open(cfg['logfile'], 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count',\n",
    "                     'wall_collisions', 'box_collisions',\n",
    "                     'endless_loops','reward', 'epsilon', 'train_loss', 'edge_threshold', 'validation'])\n",
    "\n",
    "# Counters\n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "episode_reward = 0\n",
    "endless_loops = 0\n",
    "total_loss = 0\n",
    "step_count = 0\n",
    "best_reward = np.NINF\n",
    "best_tr_reward = np.NINF\n",
    "optimizer.optimization_count = 0\n",
    "target_net_update_count = 0\n",
    "\n",
    "for episode in range(cfg['max_episodes']):\n",
    "\n",
    "    if cfg['adaptive_threshold']:\n",
    "        edge_threshold = img_processing.canny.threshold.item()\n",
    "    else:\n",
    "        edge_threshold = cfg['edge_threshold']\n",
    "\n",
    "    # Valdation loop\n",
    "    if episode_reward > best_tr_reward:\n",
    "        best_tr_reward = episode_reward\n",
    "        best_episode_so_far = True\n",
    "\n",
    "    if (episode % cfg['validate_every'] == 0) or best_episode_so_far:\n",
    "        best_episode_so_far = False\n",
    "        val_performance = validation_loop(agent,environment,img_processing,cfg)\n",
    "        val_reward = val_performance[-1]\n",
    "        print('episode {}, step count: {} wall_collisions: {}, box_collisions: {}, endless_loops: {}, total_reward: {}, threshold: {:0.4f}'.format(episode,*val_performance, edge_threshold))\n",
    "\n",
    "        # Save best model\n",
    "        if val_reward > best_reward:\n",
    "            print(\"model improved! Saving to {}\".format(cfg['model_path']))\n",
    "            best_reward = val_reward\n",
    "            torch.save(agent.policy_net.state_dict(), cfg['model_path'])\n",
    "\n",
    "        # Write validation performance to log file\n",
    "        with open(cfg['logfile'], 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([episode, *val_performance,0, 0, edge_threshold, 1])\n",
    "\n",
    "    # Write training performance to log file\n",
    "    with open(cfg['logfile'], 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,step_count,\n",
    "                         wall_collisions, box_collisions,\n",
    "                         endless_loops, episode_reward,agent.eps_threshold,total_loss, edge_threshold, 0])\n",
    "\n",
    "    # Reset counters\n",
    "    total_loss = 0 # COMMENT OUT TO REGISTER CUMULATIVE LOSS\n",
    "    wall_collisions = 0\n",
    "    box_collisions = 0\n",
    "    episode_reward = 0\n",
    "    endless_loops = 0\n",
    "    step_count = 0\n",
    "    side_steps = 0 # Side-step counter (to prevent endless loops)\n",
    "    fwd_steps = 0 # Total forward step counter (for dist from start feedback)\n",
    "\n",
    "    # Stop training after (either maximum number of steps or maximum number of episodes)\n",
    "    if optimizer.optimization_count > cfg['max_optim_steps']:\n",
    "        break\n",
    "\n",
    "\n",
    "    # Target net is updated once in a few steps (double Q-learning)\n",
    "    if optimizer.optimization_count / cfg['target_update'] >= target_net_update_count:  #steps\n",
    "        print('Target net updated!')\n",
    "        agent.update_target_net()\n",
    "        target_net_update_count += 1\n",
    "\n",
    "\n",
    "    # Reset environment at start of episode\n",
    "    # seed = torch.randint(250,(1,)).item()\n",
    "    # _, _, _ = environment.setRandomSeed(seed)\n",
    "    _, _, _ = environment.reset(cfg['training_condition'])\n",
    "\n",
    "    # Create an empty frame stack and fill it with frames\n",
    "    frame_stack = imgproc.FrameStack(stack_size=cfg['stack_size'] )\n",
    "    for _ in range(cfg['stack_size'] ):\n",
    "        _, _, frame_raw = environment.step(0)\n",
    "#             frame = img_processing(frame_raw).to(agent.device)\n",
    "#             state = frame_stack.update_with(frame)\n",
    "        frame = cv2.cvtColor(frame_raw['colors'], cv2.COLOR_BGR2GRAY)\n",
    "        stack = frame_stack.update_with(frame)\n",
    "        observation = np.expand_dims(np.stack(stack),0) # 1 X C X W X H  for C number of frames in stack\n",
    "\n",
    "#         if cfg['dist_feedback']: # Additional channel encodes distance from start\n",
    "#             state = torch.cat([state, torch.zeros(1,1,cfg['imsize'],cfg['imsize'],device=cfg['device'])], dim=1)\n",
    "\n",
    "    # Episode starts here:\n",
    "    for t in count():\n",
    "\n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        agent.policy_net.eval()\n",
    "        action = agent.select_action(observation)\n",
    "        side_steps = side_steps + 1  if action != 0 else 0\n",
    "        end, reward, frame_raw = environment.step(action.item())\n",
    "        agent_died = cfg['reset_end_is_{}'.format(end)] or side_steps > cfg['reset_after_nr_sidesteps']\n",
    "#             frame = img_processing(frame_raw).to(agent.device)\n",
    "#             next_state = frame_stack.update_with(frame) if not agent_died else None\n",
    "        frame = cv2.cvtColor(frame_raw['colors'], cv2.COLOR_BGR2GRAY)\n",
    "        stack = frame_stack.update_with(frame)\n",
    "        next_observation = np.expand_dims(np.stack(stack),0) if not agent_died else None\n",
    "\n",
    "        if action == 0:\n",
    "            fwd_steps += 1\n",
    "\n",
    "#             if cfg['dist_feedback'] and next_state is not None: # Additional channel encodes distance from start\n",
    "#                 next_state = torch.cat([next_state,fwd_steps*torch.ones(1,1,cfg['imsize'],cfg['imsize'],device=cfg['device'])], dim=1)/cfg['n_target_steps']\n",
    "\n",
    "\n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        reward *= cfg['reward_multiplier']\n",
    "\n",
    "        if side_steps > cfg['reset_after_nr_sidesteps']:\n",
    "            reward = cfg['early_stop_reward']\n",
    "\n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=agent.device,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(observation, action, next_observation, reward)\n",
    "\n",
    "\n",
    "        # 4. optimize model\n",
    "        agent.policy_net.train()\n",
    "        if len(agent.memory) > cfg['stack_size']: # cfg['replay_start_size']:\n",
    "\n",
    "            for _ in range(cfg['optimizations_per_step']):\n",
    "                state_action_values, expected_state_action_values = agent.forward()\n",
    "\n",
    "                # Compute Huber loss\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "                optimizer.optimization_count += 1\n",
    "\n",
    "        else:\n",
    "            # Do not count steps, as optimization has not started yet (delay epsilon decay)\n",
    "            agent.step_count = 0\n",
    "\n",
    "        # 5. Store performance and training measures\n",
    "        step_count += 1\n",
    "        episode_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        if side_steps > cfg['reset_after_nr_sidesteps']:\n",
    "            endless_loops +=1\n",
    "\n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if agent_died:\n",
    "            break\n",
    "        else:\n",
    "            observation = next_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducability, reset the RNG seed\n",
    "torch.manual_seed(cfg['seed'])\n",
    "\n",
    "# Write header to logfile\n",
    "with open(cfg['logfile'], 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count',\n",
    "                     'wall_collisions', 'box_collisions',\n",
    "                     'endless_loops','reward', 'epsilon', 'train_loss', 'edge_threshold', 'validation'])\n",
    "\n",
    "# Counters\n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "episode_reward = 0\n",
    "endless_loops = 0\n",
    "total_loss = 0\n",
    "step_count = 0\n",
    "best_reward = np.NINF\n",
    "best_tr_reward = np.NINF\n",
    "optimizer.optimization_count = 0\n",
    "target_net_update_count = 0\n",
    "\n",
    "for episode in range(cfg['max_episodes']):\n",
    "\n",
    "    if cfg['adaptive_threshold']:\n",
    "        edge_threshold = img_processing.canny.threshold.item()\n",
    "    else:\n",
    "        edge_threshold = cfg['edge_threshold']\n",
    "\n",
    "    # Valdation loop\n",
    "    if episode_reward > best_tr_reward:\n",
    "        best_tr_reward = episode_reward\n",
    "        best_episode_so_far = True\n",
    "\n",
    "    if (episode % cfg['validate_every'] == 0) or best_episode_so_far:\n",
    "        best_episode_so_far = False\n",
    "        val_performance = training.validation_loop(agent,environment,img_processing,cfg)\n",
    "        val_reward = val_performance[-1]\n",
    "        print('episode {}, step count: {} wall_collisions: {}, box_collisions: {}, endless_loops: {}, total_reward: {}, threshold: {:0.4f}'.format(episode,*val_performance, edge_threshold))\n",
    "\n",
    "        # Save best model\n",
    "        if val_reward > best_reward:\n",
    "            print(\"model improved! Saving to {}\".format(cfg['model_path']))\n",
    "            best_reward = val_reward\n",
    "            torch.save(agent.policy_net.state_dict(), cfg['model_path'])\n",
    "\n",
    "        # Write validation performance to log file\n",
    "        with open(cfg['logfile'], 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([episode, *val_performance,0, 0, edge_threshold, 1])\n",
    "\n",
    "    # Write training performance to log file\n",
    "    with open(cfg['logfile'], 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,step_count,\n",
    "                         wall_collisions, box_collisions,\n",
    "                         endless_loops, episode_reward,agent.eps_threshold,total_loss, edge_threshold, 0])\n",
    "\n",
    "    # Reset counters\n",
    "    total_loss = 0 # COMMENT OUT TO REGISTER CUMULATIVE LOSS\n",
    "    wall_collisions = 0\n",
    "    box_collisions = 0\n",
    "    episode_reward = 0\n",
    "    endless_loops = 0\n",
    "    step_count = 0\n",
    "    side_steps = 0 # Side-step counter (to prevent endless loops)\n",
    "    fwd_steps = 0 # Total forward step counter (for dist from start feedback)\n",
    "\n",
    "    # Stop training after (either maximum number of steps or maximum number of episodes)\n",
    "    if optimizer.optimization_count > cfg['max_optim_steps']:\n",
    "        break\n",
    "\n",
    "\n",
    "    # Target net is updated once in a few steps (double Q-learning)\n",
    "    if optimizer.optimization_count / cfg['target_update'] >= target_net_update_count:  #steps\n",
    "        print('Target net updated!')\n",
    "        agent.update_target_net()\n",
    "        target_net_update_count += 1\n",
    "\n",
    "\n",
    "    # Reset environment at start of episode\n",
    "    # seed = torch.randint(250,(1,)).item()\n",
    "    # _, _, _ = environment.setRandomSeed(seed)\n",
    "    _, _, _ = environment.reset(cfg['training_condition'])\n",
    "\n",
    "    # Create an empty frame stack and fill it with frames\n",
    "    frame_stack = imgproc.FrameStack(stack_size=cfg['stack_size'] )\n",
    "    for _ in range(cfg['stack_size'] ):\n",
    "        _, _, frame_raw = environment.step(0)\n",
    "        frame = img_processing(frame_raw).to(agent.device)\n",
    "        state = frame_stack.update_with(frame)\n",
    "\n",
    "    if cfg['dist_feedback']: # Additional channel encodes distance from start\n",
    "        state = torch.cat([state, torch.zeros(1,1,cfg['imsize'],cfg['imsize'],device=cfg['device'])], dim=1)\n",
    "\n",
    "    # Episode starts here:\n",
    "    for t in count():\n",
    "\n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        agent.policy_net.eval()\n",
    "        action = agent.select_action(state)\n",
    "        side_steps = side_steps + 1  if action != 0 else 0\n",
    "        end, reward, frame_raw = environment.step(action.item())\n",
    "        agent_died = cfg['reset_end_is_{}'.format(end)] or side_steps > cfg['reset_after_nr_sidesteps']\n",
    "        frame = img_processing(frame_raw).to(agent.device)\n",
    "        next_state = frame_stack.update_with(frame) if not agent_died else None\n",
    "\n",
    "        if action == 0:\n",
    "            fwd_steps += 1\n",
    "\n",
    "        if cfg['dist_feedback'] and next_state is not None: # Additional channel encodes distance from start\n",
    "            next_state = torch.cat([next_state,fwd_steps*torch.ones(1,1,cfg['imsize'],cfg['imsize'],device=cfg['device'])], dim=1)/cfg['n_target_steps']\n",
    "\n",
    "\n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        reward *= cfg['reward_multiplier']\n",
    "\n",
    "        if side_steps > cfg['reset_after_nr_sidesteps']:\n",
    "            reward = cfg['early_stop_reward']\n",
    "\n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=agent.device,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "\n",
    "        # 4. optimize model\n",
    "        agent.policy_net.train()\n",
    "        if len(agent.memory) > cfg['batch_size']:\n",
    "#         if len(agent.memory) > cfg['replay_start_size']:\n",
    "            for _ in range(cfg['optimizations_per_step']):\n",
    "                print(\"ready for optimization\")\n",
    "#                 raise KeyBoardInterrupt\n",
    "                \n",
    "                state_action_values, expected_state_action_values = agent.forward()\n",
    "\n",
    "                # Compute Huber loss\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "                optimizer.optimization_count += 1\n",
    "\n",
    "        else:\n",
    "            # Do not count steps, as optimization has not started yet (delay epsilon decay)\n",
    "            agent.step_count = 0\n",
    "\n",
    "        # 5. Store performance and training measures\n",
    "        step_count += 1\n",
    "        episode_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        if side_steps > cfg['reset_after_nr_sidesteps']:\n",
    "            endless_loops +=1\n",
    "\n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if agent_died:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_stack = imgproc.FrameStack()\n",
    "\n",
    "frame_stack.update_with(torch.ones(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_stack.stack[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FORWARD PASS\n",
    "print(f\"--grad before--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "state_action_values, expected_state_action_values = agent.forward()\n",
    "print(f\"--grad after--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"--grad before--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "\n",
    "\n",
    "# Compute Huber loss\n",
    "loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "total_loss += loss.item()\n",
    "\n",
    "print(f\"--grad after--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(loss, show_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--grad before--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "optimizer.zero_grad()\n",
    "print(f\"--grad after--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--grad before--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "loss.backward()\n",
    "print(f\"--grad after--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--grad before--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n",
    "optimizer.step()\n",
    "print(f\"--grad after--\\nthreshold:{img_processing.canny.threshold.grad} \\npolicy net: {agent.policy_net.conv1.weight.grad}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,1,128,128)\n",
    "w = torch.ones(1, requires_grad=True)\n",
    "\n",
    "x = x*w\n",
    "\n",
    "\n",
    "y = img_processing.simulator(x)\n",
    "loss = y.mean()\n",
    "\n",
    "torchviz.make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_processing.simulator.pMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values, expected_state_action_values = agent.forward()\n",
    "lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_processing.canny.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                state_action_values, expected_state_action_values = agent.forward()\n",
    "\n",
    "                # Compute Huber loss\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "                optimizer.optimization_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
