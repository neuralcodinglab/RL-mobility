{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# local files\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "from model import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128 #original 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_steps = 4000 \n",
    "EPS_DECAY = (EPS_START - EPS_END)/EPS_DECAY_steps\n",
    "REPLAY_START_SIZE =  1500\n",
    "TARGET_UPDATE = 10 #episodes\n",
    "DEVICE = 'cuda:0'\n",
    "LR_DQN = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# Environment parameters\n",
    "IMSIZE = 128\n",
    "STACK_SIZE = 4\n",
    "N_ACTIONS = 3\n",
    "IP  = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "PORT = 13000       # Port number that the TCP/IP interface listens to\n",
    "\n",
    "\n",
    "environment = pyClient.Environment(ip = IP, port = PORT, size = IMSIZE) \n",
    "agent = model.DoubleDQNAgent(imsize=IMSIZE,\n",
    "                 in_channels=STACK_SIZE,\n",
    "                 n_actions=N_ACTIONS,\n",
    "                 memory_capacity=12000,\n",
    "                 eps_start=EPS_START,\n",
    "                 eps_end=EPS_END,\n",
    "                 eps_delta=EPS_DECAY,\n",
    "                 gamma_discount = GAMMA,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 device=DEVICE)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(agent.policy_net.parameters(), lr = LR_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def process_frame(state_raw):\n",
    "    \"\"\" @TODO \n",
    "    - Image processing\n",
    "    - Phosphene simulation\n",
    "    - Frame stacking\n",
    "    \"\"\"\n",
    "    frame = environment.state2usableArray(state_raw)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame.astype('float32')\n",
    "    return torch.Tensor(frame / 255.).view(1,1,environment.size, environment.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, target net updated\n",
      "episode 10, target net updated\n",
      "episode 20, target net updated\n",
      "episode 30, target net updated\n",
      "episode 40, target net updated\n",
      "episode 50, target net updated\n",
      "episode 60, target net updated\n",
      "episode 70, target net updated\n",
      "episode 80, target net updated\n",
      "episode 90, target net updated\n",
      "episode 100, target net updated\n",
      "episode 110, target net updated\n",
      "episode 120, target net updated\n",
      "episode 130, target net updated\n",
      "episode 140, target net updated\n",
      "episode 150, target net updated\n",
      "episode 160, target net updated\n",
      "episode 170, target net updated\n",
      "episode 180, target net updated\n",
      "episode 190, target net updated\n",
      "episode 200, target net updated\n",
      "episode 210, target net updated\n",
      "episode 220, target net updated\n",
      "episode 230, target net updated\n",
      "episode 240, target net updated\n",
      "episode 250, target net updated\n",
      "episode 260, target net updated\n",
      "episode 270, target net updated\n",
      "episode 280, target net updated\n",
      "episode 290, target net updated\n",
      "episode 300, target net updated\n",
      "episode 310, target net updated\n",
      "episode 320, target net updated\n",
      "episode 330, target net updated\n",
      "episode 340, target net updated\n",
      "episode 350, target net updated\n",
      "episode 360, target net updated\n",
      "episode 370, target net updated\n"
     ]
    }
   ],
   "source": [
    "## Some more training parameters \n",
    "MAX_EPISODES = 500 # number of episodes (an episode ends after agent hits a box)\n",
    "MAX_STEPS  = 5e4  # number of optimization steps (each time step the model parameters are updated)\n",
    "TRAINING_CONDITION = 0 # 0: plain training, 1: complex training, 2: plain testing 3: complex testing\n",
    "LOGFILE = 'Out/test-07.csv'\n",
    "MODEL_PATH = 'Out/test-07.pth'\n",
    "\n",
    "# How to handle the different end signals\n",
    "RESET_UPON_END_SIGNAL = {0:False,  # Nothing happened\n",
    "                         1:True,   # Box collision\n",
    "                         2:False,   # Wall collision\n",
    "                         3:False}  # Reached step target\n",
    "RESET_AFTER_NR_SIDESTEPS = 5\n",
    "\n",
    "\n",
    "# Write header to logfile \n",
    "with open(LOGFILE, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count', 'wall_collisions', 'box_collisions', 'train_loss', 'reward'])\n",
    "\n",
    "# Reset counters \n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "total_reward = 0\n",
    "total_loss = 0\n",
    "\n",
    "\n",
    "for episode in range(int(MAX_EPISODES)):\n",
    "    \n",
    "    # @TODO: SAVE AFTER VALIDATION\n",
    "    torch.save(agent.policy_net.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Write performance to log file\n",
    "    with open(LOGFILE, 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,agent.step_count,\n",
    "                         wall_collisions, box_collisions, \n",
    "                         total_reward,total_loss]) \n",
    "        \n",
    "    # Stop training after \n",
    "    if agent.step_count > MAX_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Side-step counter (to prevent endless loops)\n",
    "    side_steps = 0\n",
    "    \n",
    "    \n",
    "    # Target net is updated once in a few episodes (double Q-learning)\n",
    "    if episode % TARGET_UPDATE == 0:  #episodes\n",
    "        print('episode {}, target net updated'.format(episode))\n",
    "        agent.update_target_net\n",
    "    \n",
    "    \n",
    "    # Reset environment at start of episode\n",
    "    _, _, frame_raw = environment.reset(TRAINING_CONDITION)\n",
    "    frame = process_frame(frame_raw).to(DEVICE)  \n",
    "    \n",
    "    # Create an empty frame stack and fill it with frames\n",
    "    frame_stack = utils.FrameStack(stack_size=STACK_SIZE)\n",
    "    for _ in range(STACK_SIZE):\n",
    "        _, _, frame_raw = environment.step(0)\n",
    "        frame = process_frame(frame_raw).to(DEVICE) \n",
    "        state = frame_stack.update_with(frame)\n",
    "    \n",
    "    # Episode starts here:\n",
    "    for t in count(): \n",
    "        \n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        action = agent.select_action(state)\n",
    "        side_steps = side_steps + 1  if action != 0 else 0\n",
    "        end, reward, frame_raw = environment.step(action.item())\n",
    "        agent_died = RESET_UPON_END_SIGNAL[end] or side_steps > RESET_AFTER_NR_SIDESTEPS\n",
    "        frame = process_frame(frame_raw).to(DEVICE)\n",
    "        next_state = frame_stack.update_with(frame) if not agent_died else None\n",
    "        \n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        \n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=DEVICE,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    \n",
    "        # 4. optimize model\n",
    "        if len(agent.memory) > REPLAY_START_SIZE:\n",
    "            \n",
    "            state_action_values, expected_state_action_values = agent.forward()\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\" @TODO: validation loop\"\"\"\n",
    "        else:\n",
    "            # Do not count as optimization loop\n",
    "            agent.step_count = 0\n",
    "            \n",
    "        # 5. Store performance and training measures\n",
    "        total_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if agent_died:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EXAMPLES = 10\n",
    "\n",
    "i = 0\n",
    "\n",
    "# for state, action, next_state, reward in agent.memory.memory[:EXAMPLES]:\n",
    "for state, action, next_state, reward in bad_choices[:EXAMPLES]:\n",
    "    i+=1\n",
    "    \n",
    "    plt.figure(figsize = (10,10), dpi=200)\n",
    "    img = torch.cat([state[0,t,...] for t in range(STACK_SIZE)],dim=1)\n",
    "    if next_state is not None:\n",
    "        img = torch.cat([img, next_state[0,-1,...]],dim=1)\n",
    "        plt.axvline(x=STACK_SIZE*IMSIZE,color='r')\n",
    "    plt.imshow(img.detach().cpu().numpy())\n",
    "    plt.title('Action: {}, Reward {}'.format(action.item(),reward.item()))\n",
    "    plt.axis('off')\n",
    "    plt.ylabel('frames >')\n",
    "    plt.xlabel('state | next state')\n",
    "    plt.show()\n",
    "    \n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_choices = [(state, action, next_state,reward) for (state, action, next_state,reward) in agent.memory.memory if reward<0 and action ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
