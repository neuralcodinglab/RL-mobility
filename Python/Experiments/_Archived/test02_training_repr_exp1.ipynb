{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# local files\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "from model import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128 #original 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_steps = 4000 \n",
    "EPS_DECAY = (EPS_START - EPS_END)/EPS_DECAY_steps\n",
    "REPLAY_START_SIZE =  128 # TODO PUT BACK TO 1500 #steps taken\n",
    "TARGET_UPDATE = 10 #episodes\n",
    "DEVICE = 'cpu'\n",
    "LR_DQN = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# Environment parameters\n",
    "IMSIZE = 128\n",
    "STACK_SIZE = 1\n",
    "N_ACTIONS = 3\n",
    "IP  = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "PORT = 13000       # Port number that the TCP/IP interface listens to\n",
    "\n",
    "\n",
    "environment = pyClient.Environment(ip = IP, port = PORT, size = IMSIZE) \n",
    "agent = model.DoubleDQNAgent(imsize=IMSIZE,\n",
    "                 in_channels=STACK_SIZE,\n",
    "                 n_actions=N_ACTIONS,\n",
    "                 memory_capacity=12000,\n",
    "                 eps_start=EPS_START,\n",
    "                 eps_end=EPS_END,\n",
    "                 eps_delta=EPS_DECAY,\n",
    "                 gamma_discount = GAMMA,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 device=DEVICE)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(agent.policy_net.parameters(), lr = LR_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def process_state(state_raw):\n",
    "    \"\"\" @TODO \n",
    "    - Image processing\n",
    "    - Phosphene simulation\n",
    "    - Frame stacking\n",
    "    \"\"\"\n",
    "    frame = environment.state2usableArray(state_raw)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame.astype('float32')\n",
    "    return torch.Tensor(frame / 255.).view(1,1,environment.size, environment.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, target net updated\n",
      "episode 10, target net updated\n",
      "episode 20, target net updated\n",
      "episode 30, target net updated\n",
      "episode 40, target net updated\n",
      "episode 50, target net updated\n",
      "episode 60, target net updated\n",
      "episode 70, target net updated\n",
      "episode 80, target net updated\n",
      "episode 90, target net updated\n",
      "episode 100, target net updated\n",
      "episode 110, target net updated\n",
      "episode 120, target net updated\n",
      "episode 130, target net updated\n",
      "episode 140, target net updated\n",
      "episode 150, target net updated\n",
      "episode 160, target net updated\n",
      "episode 170, target net updated\n",
      "episode 180, target net updated\n",
      "episode 190, target net updated\n",
      "episode 200, target net updated\n",
      "episode 210, target net updated\n",
      "episode 220, target net updated\n",
      "episode 230, target net updated\n",
      "episode 240, target net updated\n",
      "episode 250, target net updated\n",
      "episode 260, target net updated\n",
      "episode 270, target net updated\n",
      "episode 280, target net updated\n",
      "episode 290, target net updated\n",
      "episode 300, target net updated\n",
      "episode 310, target net updated\n",
      "episode 320, target net updated\n",
      "episode 330, target net updated\n",
      "episode 340, target net updated\n",
      "episode 350, target net updated\n",
      "episode 360, target net updated\n",
      "episode 370, target net updated\n",
      "episode 380, target net updated\n",
      "episode 390, target net updated\n",
      "episode 400, target net updated\n",
      "episode 410, target net updated\n",
      "episode 420, target net updated\n",
      "episode 430, target net updated\n",
      "episode 440, target net updated\n",
      "episode 450, target net updated\n",
      "episode 460, target net updated\n",
      "episode 470, target net updated\n",
      "episode 480, target net updated\n",
      "episode 490, target net updated\n",
      "episode 500, target net updated\n",
      "episode 510, target net updated\n",
      "episode 520, target net updated\n",
      "episode 530, target net updated\n",
      "episode 540, target net updated\n",
      "episode 550, target net updated\n",
      "episode 560, target net updated\n",
      "episode 570, target net updated\n",
      "episode 580, target net updated\n",
      "episode 590, target net updated\n",
      "episode 600, target net updated\n",
      "episode 610, target net updated\n",
      "episode 620, target net updated\n",
      "episode 630, target net updated\n",
      "episode 640, target net updated\n",
      "episode 650, target net updated\n",
      "episode 660, target net updated\n",
      "episode 670, target net updated\n",
      "episode 680, target net updated\n",
      "episode 690, target net updated\n",
      "episode 700, target net updated\n",
      "episode 710, target net updated\n",
      "episode 720, target net updated\n",
      "episode 730, target net updated\n",
      "episode 740, target net updated\n",
      "episode 750, target net updated\n",
      "episode 760, target net updated\n",
      "episode 770, target net updated\n",
      "episode 780, target net updated\n",
      "episode 790, target net updated\n",
      "episode 800, target net updated\n",
      "episode 810, target net updated\n",
      "episode 820, target net updated\n",
      "episode 830, target net updated\n",
      "episode 840, target net updated\n",
      "episode 850, target net updated\n",
      "episode 860, target net updated\n",
      "episode 870, target net updated\n",
      "episode 880, target net updated\n",
      "episode 890, target net updated\n",
      "episode 900, target net updated\n",
      "episode 910, target net updated\n",
      "episode 920, target net updated\n",
      "episode 930, target net updated\n",
      "episode 940, target net updated\n",
      "episode 950, target net updated\n",
      "episode 960, target net updated\n",
      "episode 970, target net updated\n",
      "episode 980, target net updated\n",
      "episode 990, target net updated\n",
      "episode 1000, target net updated\n",
      "episode 1010, target net updated\n",
      "episode 1020, target net updated\n",
      "episode 1030, target net updated\n",
      "episode 1040, target net updated\n",
      "episode 1050, target net updated\n",
      "episode 1060, target net updated\n",
      "episode 1070, target net updated\n",
      "episode 1080, target net updated\n",
      "episode 1090, target net updated\n",
      "episode 1100, target net updated\n",
      "episode 1110, target net updated\n",
      "episode 1120, target net updated\n",
      "episode 1130, target net updated\n",
      "episode 1140, target net updated\n",
      "episode 1150, target net updated\n",
      "episode 1160, target net updated\n",
      "episode 1170, target net updated\n",
      "episode 1180, target net updated\n",
      "episode 1190, target net updated\n",
      "episode 1200, target net updated\n",
      "episode 1210, target net updated\n",
      "episode 1220, target net updated\n",
      "episode 1230, target net updated\n",
      "episode 1240, target net updated\n",
      "episode 1250, target net updated\n",
      "episode 1260, target net updated\n",
      "episode 1270, target net updated\n",
      "episode 1280, target net updated\n",
      "episode 1290, target net updated\n",
      "episode 1300, target net updated\n",
      "episode 1310, target net updated\n",
      "episode 1320, target net updated\n",
      "episode 1330, target net updated\n",
      "episode 1340, target net updated\n",
      "episode 1350, target net updated\n",
      "episode 1360, target net updated\n",
      "episode 1370, target net updated\n",
      "episode 1380, target net updated\n",
      "episode 1390, target net updated\n",
      "episode 1400, target net updated\n",
      "episode 1410, target net updated\n",
      "episode 1420, target net updated\n",
      "episode 1430, target net updated\n",
      "episode 1440, target net updated\n",
      "episode 1450, target net updated\n",
      "episode 1460, target net updated\n",
      "episode 1470, target net updated\n",
      "episode 1480, target net updated\n",
      "episode 1490, target net updated\n",
      "episode 1500, target net updated\n",
      "episode 1510, target net updated\n",
      "episode 1520, target net updated\n",
      "episode 1530, target net updated\n",
      "episode 1540, target net updated\n",
      "episode 1550, target net updated\n",
      "episode 1560, target net updated\n",
      "episode 1570, target net updated\n",
      "episode 1580, target net updated\n",
      "episode 1590, target net updated\n",
      "episode 1600, target net updated\n",
      "episode 1610, target net updated\n",
      "episode 1620, target net updated\n",
      "episode 1630, target net updated\n",
      "episode 1640, target net updated\n",
      "episode 1650, target net updated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dad67c5b04d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mRESET_UPON_END_SIGNAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jaap/rl-mobility/Python/pyClient.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetRandomSeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jaap/rl-mobility/Python/pyClient.py\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdata\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSG_WAITALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mend\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Some more training parameters \n",
    "MAX_EPISODES = 1e4 # number of episodes (an episode ends after agent hits a box)\n",
    "MAX_STEPS  = 5e4  # number of optimization steps (each time step the model parameters are updated)\n",
    "TRAINING_CONDITION = 0 # 0: plain training, 1: complex training, 2: plain testing 3: complex testing\n",
    "LOGFILE = 'Out/test-02.csv'\n",
    "MODEL_PATH = 'Out/test-02.pth'\n",
    "\n",
    "# How to handle the different end signals\n",
    "RESET_UPON_END_SIGNAL = {0:False,  # Nothing happened\n",
    "                         1:True,   # Box collision\n",
    "                         2:True,   # Wall collision\n",
    "                         3:False}  # Reached step target\n",
    "\n",
    "\n",
    "# Write header to logfile \n",
    "with open(LOGFILE, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count', 'wall_collisions', 'box_collisions' 'train_loss', 'reward'])\n",
    "\n",
    "# Reset counters \n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "total_reward = 0\n",
    "total_loss = 0\n",
    "\n",
    "\n",
    "for episode in range(int(MAX_EPISODES)):\n",
    "    \n",
    "    # @TODO: SAVE AFTER VALIDATION\n",
    "    torch.save(agent.policy_net.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Write performance to log file\n",
    "    with open(LOGFILE, 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,agent.step_count,\n",
    "                         wall_collisions, box_collisions, \n",
    "                         total_reward,total_loss]) \n",
    "    \n",
    "    \n",
    "    # Stop training after \n",
    "    if agent.step_count > MAX_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Target net is updated once in a few episodes (double Q-learning)\n",
    "    if episode % TARGET_UPDATE == 0:  #episodes\n",
    "        print('episode {}, target net updated'.format(episode))\n",
    "        agent.update_target_net\n",
    "    \n",
    "    \n",
    "    # Reset environment at start of episode\n",
    "    _, _, state_raw = environment.reset(TRAINING_CONDITION)\n",
    "\n",
    "    # Episode starts here:\n",
    "    for t in count(): \n",
    "        \n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        state = process_state(state_raw).to(DEVICE)\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        end, reward, next_state_raw = environment.step(action.item())\n",
    "        next_state = process_state(next_state_raw).to(DEVICE) if not RESET_UPON_END_SIGNAL[end] else None\n",
    "        \n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        \n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=DEVICE,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    \n",
    "        # 4. optimize model\n",
    "        if len(agent.memory) > REPLAY_START_SIZE:\n",
    "            \n",
    "            state_action_values, expected_state_action_values = agent.forward()\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\" @TODO: validation loop\"\"\"\n",
    "        else:\n",
    "            # Do not count as optimization loop\n",
    "            agent.step_count = 0\n",
    "            \n",
    "        # 5. Store performance and training measures\n",
    "        total_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        \n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if RESET_UPON_END_SIGNAL[end]:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
