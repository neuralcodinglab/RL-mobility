{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# local files\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "from model import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128 #original 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_steps = 4000 \n",
    "EPS_DECAY = (EPS_START - EPS_END)/EPS_DECAY_steps\n",
    "REPLAY_START_SIZE =  128 # TODO PUT BACK TO 1500 #steps taken\n",
    "TARGET_UPDATE = 10 #episodes\n",
    "DEVICE = 'cpu'\n",
    "LR_DQN = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# Environment parameters\n",
    "IMSIZE = 128\n",
    "STACK_SIZE = 1\n",
    "N_ACTIONS = 3\n",
    "IP  = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "PORT = 13000       # Port number that the TCP/IP interface listens to\n",
    "\n",
    "\n",
    "environment = pyClient.Environment(ip = IP, port = PORT, size = IMSIZE) \n",
    "agent = model.DoubleDQNAgent(imsize=IMSIZE,\n",
    "                 in_channels=STACK_SIZE,\n",
    "                 n_actions=N_ACTIONS,\n",
    "                 memory_capacity=12000,\n",
    "                 eps_start=EPS_START,\n",
    "                 eps_end=EPS_END,\n",
    "                 eps_delta=EPS_DECAY,\n",
    "                 gamma_discount = GAMMA,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 device=DEVICE)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(agent.policy_net.parameters(), lr = LR_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def process_state(state_raw):\n",
    "    \"\"\" @TODO \n",
    "    - Image processing\n",
    "    - Phosphene simulation\n",
    "    - Frame stacking\n",
    "    \"\"\"\n",
    "    frame = environment.state2usableArray(state_raw)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame.astype('float32')\n",
    "    return torch.Tensor(frame / 255.).view(1,1,environment.size, environment.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, target net updated\n",
      "episode 10, target net updated\n",
      "episode 20, target net updated\n"
     ]
    }
   ],
   "source": [
    "## Some more training parameters \n",
    "MAX_EPISODES = 1e4 # number of episodes (an episode ends after agent hits a box)\n",
    "MAX_STEPS  = 5e4  # number of optimization steps (each time step the model parameters are updated)\n",
    "TRAINING_CONDITION = 0 # 0: plain training, 1: complex training, 2: plain testing 3: complex testing\n",
    "LOGFILE = 'Out/test-02.csv'\n",
    "MODEL_PATH = 'Out/test-02.pth'\n",
    "\n",
    "# How to handle the different end signals\n",
    "RESET_UPON_END_SIGNAL = {0:False,  # Nothing happened\n",
    "                         1:True,   # Box collision\n",
    "                         2:True,   # Wall collision\n",
    "                         3:False}  # Reached step target\n",
    "\n",
    "\n",
    "# Write header to logfile \n",
    "with open(LOGFILE, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count', 'wall_collisions', 'box_collisions' 'train_loss', 'reward'])\n",
    "\n",
    "# Reset counters \n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "total_reward = 0\n",
    "total_loss = 0\n",
    "\n",
    "\n",
    "for episode in range(int(MAX_EPISODES)):\n",
    "    \n",
    "    # @TODO: SAVE AFTER VALIDATION\n",
    "    torch.save(agent.policy_net.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Write performance to log file\n",
    "    with open(LOGFILE, 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,agent.step_count,\n",
    "                         wall_collisions, box_collisions, \n",
    "                         total_reward,total_loss]) \n",
    "    \n",
    "    \n",
    "    # Stop training after \n",
    "    if agent.step_count > MAX_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Target net is updated once in a few episodes (double Q-learning)\n",
    "    if episode % TARGET_UPDATE == 0:  #episodes\n",
    "        print('episode {}, target net updated'.format(episode))\n",
    "        agent.update_target_net\n",
    "    \n",
    "    \n",
    "    # Reset environment at start of episode\n",
    "    _, _, state_raw = environment.reset(TRAINING_CONDITION)\n",
    "\n",
    "    # Episode starts here:\n",
    "    for t in count(): \n",
    "        \n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        state = process_state(state_raw).to(DEVICE)\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        end, reward, next_state_raw = environment.step(action.item())\n",
    "        next_state = process_state(next_state_raw).to(DEVICE) if not RESET_UPON_END_SIGNAL[end] else None\n",
    "        \n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        \n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=DEVICE,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    \n",
    "        # 4. optimize model\n",
    "        if len(agent.memory) > REPLAY_START_SIZE:\n",
    "            \n",
    "            state_action_values, expected_state_action_values = agent.forward()\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\" @TODO: validation loop\"\"\"\n",
    "        else:\n",
    "            # Do not count as optimization loop\n",
    "            agent.step_count = 0\n",
    "            \n",
    "        # 5. Store performance and training measures\n",
    "        total_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        \n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if RESET_UPON_END_SIGNAL[end]:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
