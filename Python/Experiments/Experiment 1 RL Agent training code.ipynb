{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pyClientRLagentPytorch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, input_channels, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilities\n",
    "#sound wave to be played when training is complete\n",
    "wave= np.sin(2.6*np.pi*400*np.arange(10000)/10000)\n",
    "\n",
    "def make_stack(frame, stack_size):\n",
    "    stack_of_frames_initial = np.zeros((size, size, stack_size))\n",
    "    for i in range(stack_size):\n",
    "        stack_of_frames_initial[:,:,i] = frame\n",
    "    return stack_of_frames_initial\n",
    "\n",
    "def update_stack(stack, new_frame, stack_size):\n",
    "    stack2 = stack.copy()\n",
    "    for i in range(stack_size - 1):\n",
    "        stack[:,:,i] = stack2[:,:,i+1].copy()\n",
    "    stack[:,:,stack_size-1] = new_frame\n",
    "    return stack \n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = max(EPS_END, (EPS_START-(EPS_DECAY*steps_done)))\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def state_to_phosphenes(raw_state, phosphenes_input, sigma, threshold_high, threshold_low, phosphene_resolution, simulator):\n",
    "    image_array = environment.state2usableArray(raw_state)\n",
    "    ksize = 11 #np.round(4*sigma)+1\n",
    "    blurred = cv2.GaussianBlur(image_array,(ksize,ksize),sigma)\n",
    "    canny = cv2.Canny(blurred,threshold_low,threshold_high)   \n",
    "    phosphenes = simulator(activation_mask=canny)\n",
    "    if phosphenes_input:\n",
    "        frame = phosphenes\n",
    "    else:\n",
    "        frame = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "        frame = frame.astype('float32')\n",
    "    # normalize to the range 0-1\n",
    "    frame /= 255.0\n",
    "    return image_array, blurred, canny, phosphenes, frame\n",
    "\n",
    "    \n",
    "def get_current_date():\n",
    "    date = datetime.datetime.now()\n",
    "    return date.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "def get_model_name(v_b, v_w, v_l, v_t, i_e, t_e, t_r,impr):\n",
    "    model_name_string = get_current_date()+\"_Phos\"+str(PHOSPHENES)+\"_Res\"+str(phosphene_resolution[0])+\"_Ep\"+\\\n",
    "        str(i_e)+\"_BoxBumps\"+str(v_b)+\"_WallBumps\"+str(v_w)+\"_Loops\"+str(v_l)+\"_Steps\"+str(v_t)+\"_TotalErrors\"+\\\n",
    "        str(t_e)+\"_TotalReward\"+str(t_r)+impr+\".pth\"\n",
    "    return model_name_string\n",
    "\n",
    "episode_rewards = []\n",
    "loss_list = []\n",
    "RUNNING_AVERAGE_NR = 20\n",
    "\n",
    "def plot_rewards():\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "    plt.figure(1, figsize=(16,4))\n",
    "    plt.clf()\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), 'b', label = 'reward')\n",
    "    # Take 20 episode averages and plot them too\n",
    "    if len(rewards_t) >= RUNNING_AVERAGE_NR:\n",
    "        means_r = rewards_t.unfold(0, RUNNING_AVERAGE_NR, 1).mean(1).view(-1)\n",
    "        means_r = torch.cat((torch.zeros(RUNNING_AVERAGE_NR - 1), means_r))\n",
    "        plt.plot(means_r.numpy(), 'c', label = str(RUNNING_AVERAGE_NR) +' episodes average reward')\n",
    "    plt.legend()\n",
    "    plt.pause(0.0001)  # pause a bit so that plots are updated\n",
    "\n",
    "def plot_loss():\n",
    "    plt.figure(2, figsize=(16,4))\n",
    "    plt.clf()\n",
    "    loss_t = torch.tensor(loss_list, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss_t.numpy(), 'g', label = 'loss')\n",
    "    # Take 20 episode averages and plot them too\n",
    "    if len(loss_t) >= RUNNING_AVERAGE_NR:\n",
    "        loss_running_average = loss_t.unfold(0, RUNNING_AVERAGE_NR, 1).mean(1).view(-1)\n",
    "        loss_running_average = torch.cat((torch.zeros(RUNNING_AVERAGE_NR - 1), loss_running_average))\n",
    "        plt.plot(loss_running_average.numpy(), 'm', label = str(RUNNING_AVERAGE_NR) +' episodes average loss')\n",
    "    plt.legend()\n",
    "    plt.pause(0.0001)  # pause a bit so that plots are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single step for the optimization\n",
    "def optimize_model(loss_list_nr_ep):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    loss_list_nr_ep +=loss.item()\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss_list_nr_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(validation_episodes, i_episode, frame_size):\n",
    "    environment.reset() #this ensures the validation seed will be set correctly with the next environment.reset_validation()\n",
    "    numpy_training_seed = np.random.randint(0,9999999) #generate new seed for after validation\n",
    "    np.random.seed(validation_np_random_seed) #set the random seed to the validation seed\n",
    "    policy_net.eval()\n",
    "    last_loop = False\n",
    "    box_bump_count = 0\n",
    "    wall_bump_count_val = 0\n",
    "    loop_count = 0\n",
    "    total_steps_taken = 0\n",
    "    reward_val = 0\n",
    "    for ep in range(validation_episodes):\n",
    "        if PHOSPHENES:\n",
    "            simulator = utils.phosphene_simulator(phosphene_resolution, (screen_height, screen_width))\n",
    "        else:\n",
    "            simulator = utils.phosphene_simulator((50,50), (screen_height, screen_width))#dummy simulator\n",
    "        print(\"Loop: \"+str(ep + 1))\n",
    "        wall_piece_hit = False\n",
    "        steps_not_moved_forward = 0\n",
    "        end, reward, state_raw = environment.reset_val()\n",
    "        original, blurred, canny, phosphene_frame, frame = state_to_phosphenes(state_raw, PHOSPHENES, SIGMA, threshold_high, threshold_low, phosphene_resolution, simulator)\n",
    "        \n",
    "        stack_of_frames = make_stack(frame, stack_size)\n",
    "        state = stack_of_frames.transpose((2, 0, 1))\n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(device, dtype=torch.float)\n",
    "        \n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)\n",
    "            move = action.item()\n",
    "\n",
    "            if move == 0:\n",
    "                steps_not_moved_forward = 0\n",
    "            else:\n",
    "                steps_not_moved_forward += 1\n",
    "\n",
    "            if steps_not_moved_forward >= 11:\n",
    "                loop_count += 1\n",
    "                move = 0\n",
    "                steps_not_moved_forward = 0\n",
    "\n",
    "            end, reward, next_state_raw = environment.step_val(move)\n",
    "\n",
    "            original, blurred, canny, phosphene_frame, next_frame = state_to_phosphenes(next_state_raw, PHOSPHENES, SIGMA, threshold_high, threshold_low, phosphene_resolution, simulator)\n",
    "            stack_of_frames = update_stack(stack_of_frames, next_frame, stack_size)\n",
    "            state = stack_of_frames.transpose((2, 0, 1))\n",
    "            state = torch.from_numpy(state).unsqueeze(0).to(device, dtype=torch.float)\n",
    "                        \n",
    "            if reward == 120:\n",
    "                box_bump_count +=1\n",
    "            if reward == 110:\n",
    "                if wall_piece_hit == False:\n",
    "                    wall_bump_count_val += 1\n",
    "                    wall_piece_hit = True\n",
    "            else:\n",
    "                wall_piece_hit = False\n",
    "                \n",
    "            if reward > 100:\n",
    "                reward_val += -(reward -100)\n",
    "            else:\n",
    "                reward_val += reward\n",
    "                \n",
    "            if end == 2:\n",
    "                total_steps_taken += t\n",
    "                break\n",
    "    \n",
    "    total_errors = box_bump_count + wall_bump_count_val + loop_count      \n",
    "    policy_net.train()\n",
    "    np.random.seed(numpy_training_seed) #reset randomness with seed generated before the validation seed\n",
    "    return box_bump_count/validation_episodes, wall_bump_count_val/validation_episodes, loop_count/validation_episodes, total_steps_taken/validation_episodes, total_errors/validation_episodes, reward_val/validation_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 62, 62]           1,616\n",
      "       BatchNorm2d-2           [-1, 16, 62, 62]              32\n",
      "            Conv2d-3           [-1, 32, 29, 29]          12,832\n",
      "       BatchNorm2d-4           [-1, 32, 29, 29]              64\n",
      "            Conv2d-5           [-1, 32, 13, 13]          25,632\n",
      "       BatchNorm2d-6           [-1, 32, 13, 13]              64\n",
      "            Linear-7                    [-1, 3]          16,227\n",
      "================================================================\n",
      "Total params: 56,467\n",
      "Trainable params: 56,467\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 1.43\n",
      "Params size (MB): 0.22\n",
      "Estimated Total Size (MB): 1.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "BATCH_SIZE = 128 #original 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_steps = 4000 \n",
    "EPS_DECAY = (EPS_START - EPS_END)/EPS_DECAY_steps\n",
    "REPLAY_START_SIZE = 1500 #steps taken\n",
    "TARGET_UPDATE = 10 #episodes\n",
    "\n",
    "validation_start = 200 #episodes\n",
    "validation_interval = 10 #episodes\n",
    "validation_loops = 5\n",
    "\n",
    "stack_size = 4\n",
    "PHOSPHENES = False #use the phosphene representation as input or not\n",
    "\n",
    "size       = 128\n",
    "# screenshot size\n",
    "screen_height = screen_width = size\n",
    "\n",
    "input_channels = stack_size\n",
    "n_actions = 3\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, input_channels, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, input_channels, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "summary(policy_net, (input_channels, size, size))\n",
    "\n",
    "lr_dqn = 0.01\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr = lr_dqn)\n",
    "memory = ReplayMemory(12000) \n",
    "#working combos rtx 2080, 8GB vram:\n",
    "# size / memory / batch / phosphenes/ stack_size / neural_network\n",
    "# 128  / 12000  / 128   / yes       / 4          / original DQN (can go a bit higher)\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unity environment\n",
    "ip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "port       = 13000       # Port number that the TCP/IP interface listens to\n",
    "timescale  = 1           # update step scale for unity\n",
    "\n",
    "environment = pyClientRLagentPytorch.Environment(ip = ip, port = port, size = size, timescale = timescale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Jupiter_Files\\Current\\Research_project\\RL_agents_phosphene_master_thesis\\utils.py:15: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.phosphene_spacing = np.divide(size,phosphene_resolution)\n",
      "E:\\Jupiter_Files\\Current\\Research_project\\RL_agents_phosphene_master_thesis\\utils.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  phosphene_spacing = np.divide(size,phosphene_resolution)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.1) c:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\filterengine.hpp:363: error: (-215:Assertion failed) anchor.inside(Rect(0, 0, ksize.width, ksize.height)) in function 'cv::normalizeAnchor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c71e083b3318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Initialize the environment and state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m#phosphene simulator aanroepen zonder numpy seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0msimulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphosphene_simulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphosphene_resolution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mreward_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mloss_list_nr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Jupiter_Files\\Current\\Research_project\\RL_agents_phosphene_master_thesis\\utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, phosphene_resolution, size, jitter, intensity_var, aperture, sigma, custom_grid)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maperture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maperture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStructuringElement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMORPH_ELLIPSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maperture\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maperture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11\u001b[0m \u001b[1;31m#np.round(4*sigma+1).astype(int) # rule of thumb: choose k_size>3*sigma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.0.1) c:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\filterengine.hpp:363: error: (-215:Assertion failed) anchor.inside(Rect(0, 0, ksize.width, ksize.height)) in function 'cv::normalizeAnchor'\n"
     ]
    }
   ],
   "source": [
    "#change here and in unity\n",
    "COMPLEX = 0\n",
    "\n",
    "SIGMA = 1.2 \n",
    "threshold_high = 50 \n",
    "threshold_low = int(0.5 * threshold_high)\n",
    "if(PHOSPHENES):\n",
    "    phosphene_resolution= (50,50)\n",
    "else:\n",
    "    phosphene_resolution= (0,0)\n",
    "\n",
    "num_episodes = 401\n",
    "\n",
    "PATH = \"./Models/\"+get_current_date()+\"_Phos\"+str(PHOSPHENES)+\"_Res\"+str(phosphene_resolution[0]) + \"_Complex\" + str(COMPLEX)\n",
    "os.mkdir(PATH)\n",
    "\n",
    "frame_width = 1024\n",
    "frame_height = 1024\n",
    "frame_size = (frame_width, frame_height)\n",
    "\n",
    "\n",
    "validation_np_random_seed = 796580\n",
    "memory_count = 0\n",
    "val_least_bumps = 10000\n",
    "val_least_steps = 10000\n",
    "val_least_wall_bumps = 10000\n",
    "val_least_loops = 10000\n",
    "val_least_errors = 40000\n",
    "val_best_reward = 0\n",
    "val_best_episode = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    #phosphene simulator aanroepen zonder numpy seed\n",
    "    if PHOSPHENES:\n",
    "        simulator = utils.phosphene_simulator(phosphene_resolution, (screen_height, screen_width))\n",
    "    else:\n",
    "        simulator = utils.phosphene_simulator((50,50), (screen_height, screen_width))#dummy simulator\n",
    "    reward_count = 0\n",
    "    loss_list_nr = 0\n",
    "    end, reward, state_raw = environment.reset()\n",
    "    original, blurred, canny, phosphene_frame, frame = state_to_phosphenes(state_raw, PHOSPHENES, SIGMA, threshold_high, threshold_low, phosphene_resolution, simulator)\n",
    "    \n",
    "    stack_of_frames = make_stack(frame, stack_size)\n",
    "    state = stack_of_frames.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state).unsqueeze(0).to(device, dtype=torch.float)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        \n",
    "        end, reward, next_state_raw = environment.step(action.item())\n",
    "   \n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        if end == 0 or end == 2:\n",
    "            original, blurred, canny, phosphene_frame, next_frame = state_to_phosphenes(next_state_raw, PHOSPHENES, SIGMA, threshold_high, threshold_low, phosphene_resolution, simulator)\n",
    "            stack_of_frames = update_stack(stack_of_frames, next_frame, stack_size)\n",
    "            next_state = stack_of_frames.transpose((2, 0, 1))\n",
    "            next_state = torch.from_numpy(next_state).unsqueeze(0).to(device, dtype=torch.float)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network) when memory is filled enough\n",
    "        if (memory_count+t) > REPLAY_START_SIZE:\n",
    "            loss_list_nr = optimize_model(loss_list_nr)\n",
    "        else:\n",
    "            steps_done = 0\n",
    "        \n",
    "        reward_count += int(reward)\n",
    "            \n",
    "        if end != 0:\n",
    "            memory_count += (t + 1)\n",
    "            loss_list.append(loss_list_nr/(t+1))\n",
    "            episode_rewards.append(reward_count)\n",
    "            plot_rewards()\n",
    "            plot_loss()\n",
    "            plt.imshow(next_frame, cmap='gray')\n",
    "            plt.show()\n",
    "            print(memory_count)\n",
    "            break\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0 and memory_count >= REPLAY_START_SIZE:\n",
    "        print(i_episode)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if i_episode % validation_interval == 0 and i_episode >= validation_start:\n",
    "        if is_ipython:\n",
    "            display.clear_output(wait=True)\n",
    "        improved = \"\"\n",
    "        print(\"Validating episode \" +str(i_episode)+\"..\" + '\\n')\n",
    "        print(max(EPS_END, (EPS_START-(EPS_DECAY*steps_done))))\n",
    "        #start the validation session\n",
    "        val_box, val_wall, val_loop, val_time, val_total_errors, val_total_reward = validation_loop(validation_loops, i_episode, frame_size)\n",
    "        print(\"\")\n",
    "        print(\"Previous best episode: \" + str(val_best_episode))\n",
    "        print(\"Previous least box bumps val: \" + str(val_least_bumps))\n",
    "        print(\"Previous least wall bumps val: \" + str(val_least_wall_bumps))\n",
    "        print(\"Previous least loops val: \" + str(val_least_loops))\n",
    "        print(\"Previous least steps val: \" + str(val_least_steps))\n",
    "        print(\"Previous least total errors val: \" + str(val_least_errors))\n",
    "        print(\"Previous best reward val: \" + str(val_best_reward) + '\\n')\n",
    "        \n",
    "        print(\"Current episode: \" + str(i_episode))\n",
    "        print(\"Current box bumps val: \" + str(val_box))\n",
    "        print(\"Current wall bumps vall: \" + str(val_wall))\n",
    "        print(\"Current loops val: \" + str(val_loop))\n",
    "        print(\"Current steps val: \" + str(val_time))\n",
    "        print(\"Current total errors val: \" + str(val_total_errors))\n",
    "        print(\"Current total reward val: \" + str(val_total_reward) + '\\n')\n",
    "        \n",
    "        if val_total_reward > val_best_reward:\n",
    "            print(\"Agent improved because the reward increased!\")\n",
    "            val_least_bumps = val_box\n",
    "            val_least_wall_bumps = val_wall\n",
    "            val_least_loops = val_loop\n",
    "            val_least_steps = val_time\n",
    "            val_best_episode = i_episode\n",
    "            val_least_errors = val_total_errors\n",
    "            val_best_reward = val_total_reward\n",
    "            improved = \"_Improved\"\n",
    "        elif val_total_reward == val_best_reward:\n",
    "            if val_total_errors < val_least_errors:\n",
    "                print(\"Agent improved because it made less errors with equal reward!\")\n",
    "                val_least_bumps = val_box\n",
    "                val_least_wall_bumps = val_wall\n",
    "                val_least_loops = val_loop\n",
    "                val_least_steps = val_time\n",
    "                val_best_episode = i_episode\n",
    "                val_least_errors = val_total_errors\n",
    "                val_best_reward = val_total_reward\n",
    "                improved = \"_Improved\"\n",
    "            elif val_total_errors == val_least_errors:\n",
    "                if val_time < val_least_steps:\n",
    "                    print(\"Agent improved because it took less steps with equal reward and errors!\")\n",
    "                    val_least_bumps = val_box\n",
    "                    val_least_wall_bumps = val_wall\n",
    "                    val_least_loops = val_loop\n",
    "                    val_least_steps = val_time\n",
    "                    val_best_episode = i_episode\n",
    "                    val_least_errors = val_total_errors\n",
    "                    val_best_reward = val_total_reward\n",
    "                    improved = \"_Improved\"\n",
    "\n",
    "        model_name = get_model_name(val_box, val_wall, val_loop, val_time, i_episode, val_total_errors, val_total_reward,improved)\n",
    "        torch.save(policy_net.state_dict(), PATH + \"/\" + model_name)\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards()\n",
    "plot_loss()\n",
    "#play sound when done\n",
    "Audio(wave, rate=10000, autoplay=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:masterthesis]",
   "language": "python",
   "name": "conda-env-masterthesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
