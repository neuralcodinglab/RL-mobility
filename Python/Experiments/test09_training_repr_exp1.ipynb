{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from IPython.display import Audio\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# local files\n",
    "sys.path.insert(0, '../')\n",
    "import pyClient\n",
    "import utils\n",
    "import model\n",
    "from model import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128 #original 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_steps = 4000 \n",
    "EPS_DECAY = (EPS_START - EPS_END)/EPS_DECAY_steps\n",
    "REPLAY_START_SIZE =  1500\n",
    "TARGET_UPDATE = 10 #episodes\n",
    "DEVICE = 'cuda:0'\n",
    "LR_DQN = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# Environment parameters\n",
    "IMSIZE = 128\n",
    "STACK_SIZE = 4\n",
    "N_ACTIONS = 3\n",
    "IP  = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "PORT = 13000       # Port number that the TCP/IP interface listens to\n",
    "\n",
    "\n",
    "environment = pyClient.Environment(ip = IP, port = PORT, size = IMSIZE) \n",
    "agent = model.DoubleDQNAgent(imsize=IMSIZE,\n",
    "                 in_channels=STACK_SIZE,\n",
    "                 n_actions=N_ACTIONS,\n",
    "                 memory_capacity=12000,\n",
    "                 eps_start=EPS_START,\n",
    "                 eps_end=EPS_END,\n",
    "                 eps_delta=EPS_DECAY,\n",
    "                 gamma_discount = GAMMA,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 device=DEVICE)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(agent.policy_net.parameters(), lr = LR_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def process_frame(state_raw):\n",
    "    \"\"\" @TODO \n",
    "    - Image processing\n",
    "    - Phosphene simulation\n",
    "    - Frame stacking\n",
    "    \"\"\"\n",
    "    frame = environment.state2usableArray(state_raw)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame.astype('float32')\n",
    "    return torch.Tensor(frame / 255.).view(1,1,environment.size, environment.size)\n",
    "\n",
    "def validation_loop(agent,environment,img_processing, val_seeds=[251,252,253,254,255]):\n",
    "    # How to handle the different end signals\n",
    "    RESET_UPON_END_SIGNAL = {0:False,  # Nothing happened\n",
    "                             1:False,   # Box collision\n",
    "                             2:False,   # Wall collision\n",
    "                             3:True}    # Reached step target\n",
    "    RESET_AFTER_NR_SIDESTEPS = 5\n",
    "    \n",
    "    # Set nn.module to evaluation mode\n",
    "    agent.policy_net.eval()\n",
    "    \n",
    "    # Reset counters \n",
    "    wall_collisions = 0\n",
    "    box_collisions = 0\n",
    "    total_reward = 0\n",
    "    endless_loops = 0\n",
    "    step_count = 0\n",
    "    \n",
    "\n",
    "    for seed in val_seeds:\n",
    "\n",
    "        # Reset environment at start of episode\n",
    "        _, _, _ = environment.setRandomSeed(seed)\n",
    "        _, _, _ = environment.reset(environment.training_condition)\n",
    "\n",
    "        \n",
    "        # Create an empty frame stack and fill it with frames\n",
    "        frame_stack = utils.FrameStack(stack_size=STACK_SIZE)\n",
    "        for _ in range(STACK_SIZE):\n",
    "            _, _, frame_raw = environment.step(0)\n",
    "            frame = img_processing(frame_raw).to(agent.device) \n",
    "            state = frame_stack.update_with(frame)\n",
    "        \n",
    "        side_steps = 0\n",
    "\n",
    "        # Episode starts here:\n",
    "        for t in count(): \n",
    "\n",
    "            # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "            action = agent.select_action(state,validation=True)\n",
    "            end, reward, next_state_raw = environment.step(action.item())\n",
    "            frame = img_processing(next_state_raw).to(agent.device) \n",
    "            next_state = frame_stack.update_with(frame)\n",
    "            side_steps = side_steps + 1  if action != 0 else 0\n",
    "            \n",
    "            # 2. interpret reward\n",
    "            if reward > 100:\n",
    "                reward = -(reward -100)\n",
    "\n",
    "            # 3. Store performance and training measures\n",
    "            total_reward += reward;\n",
    "            if end == 1:\n",
    "                box_collisions += 1\n",
    "            if end == 2:\n",
    "                wall_collisions +=1\n",
    "\n",
    "            # 4. the episode ends here after reaching step target or too many side steps\n",
    "            if side_steps>RESET_AFTER_NR_SIDESTEPS:\n",
    "                endless_loops += 1\n",
    "                step_count += t\n",
    "                break\n",
    "            elif RESET_UPON_END_SIGNAL[end]:\n",
    "                step_count += t\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "                \n",
    "    agent.policy_net.train()\n",
    "    print('step count {} wall_collisions: {}, box_collisions: {}, endless_loops: {}, total_reward: {}'.format(step_count, wall_collisions, box_collisions, endless_loops, total_reward))\n",
    "    return step_count, wall_collisions, box_collisions, endless_loops, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step count 104 wall_collisions: 38, box_collisions: 3, endless_loops: 5, total_reward: 31\n",
      "new best model\n",
      "episode 0, target net updated\n",
      "episode 10, target net updated\n",
      "episode 20, target net updated\n",
      "episode 30, target net updated\n",
      "episode 40, target net updated\n",
      "step count 25 wall_collisions: 8, box_collisions: 0, endless_loops: 5, total_reward: -102\n",
      "episode 50, target net updated\n"
     ]
    }
   ],
   "source": [
    "## Some more training parameters \n",
    "MAX_EPISODES = 500 # number of episodes (an episode ends after agent hits a box)\n",
    "MAX_STEPS  = 5e4  # number of optimization steps (each time step the model parameters are updated)\n",
    "TRAINING_CONDITION = 0 # 0: plain training, 1: complex training, 2: plain testing 3: complex testing\n",
    "LOGFILE = 'Out/test-09.csv'\n",
    "MODEL_PATH = 'Out/test-09.pth'\n",
    "SEED = 0\n",
    "\n",
    "# How to handle the different end signals\n",
    "RESET_UPON_END_SIGNAL = {0:False,  # Nothing happened\n",
    "                         1:True,   # Box collision\n",
    "                         2:False,   # Wall collision\n",
    "                         3:True}  # Reached step target\n",
    "RESET_AFTER_NR_SIDESTEPS = 5\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "environment.training_condition = TRAINING_CONDITION\n",
    "\n",
    "# Write header to logfile \n",
    "with open(LOGFILE, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['episode','step_count', \n",
    "                     'wall_collisions', 'box_collisions',\n",
    "                     'endless_loops','reward', 'train_loss', 'validation'])\n",
    "\n",
    "\n",
    "# Counters \n",
    "wall_collisions = 0\n",
    "box_collisions = 0\n",
    "episode_reward = 0\n",
    "endless_loops = 0\n",
    "total_loss = 0\n",
    "step_count = 0\n",
    "best_reward = 0\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(int(MAX_EPISODES)):\n",
    "    \n",
    "    # @TODO: SAVE AFTER VALIDATION\n",
    "    if episode % 50 == 0:\n",
    "        val_performance = validation_loop(agent,environment,process_frame)\n",
    "        val_reward = val_performance[-1]\n",
    "        if val_reward > best_reward:\n",
    "            print(\"new best model\")\n",
    "            best_reward = val_reward\n",
    "            torch.save(agent.policy_net.state_dict(), MODEL_PATH)\n",
    "            \n",
    "        # Write validation performance to log file\n",
    "        with open(LOGFILE, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([episode, *val_performance, 0, 1])        \n",
    "\n",
    "    # Write training performance to log file\n",
    "    with open(LOGFILE, 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([episode,step_count,\n",
    "                         wall_collisions, box_collisions, \n",
    "                         endless_loops, episode_reward,total_loss, 0]) \n",
    "    \n",
    "    # Reset counters \n",
    "    wall_collisions = 0\n",
    "    box_collisions = 0\n",
    "    episode_reward = 0\n",
    "    endless_loops = 0\n",
    "    step_count = 0\n",
    "\n",
    "    # Stop training after \n",
    "    if agent.step_count > MAX_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Side-step counter (to prevent endless loops)\n",
    "    side_steps = 0\n",
    "    \n",
    "    \n",
    "    # Target net is updated once in a few episodes (double Q-learning)\n",
    "    if episode % TARGET_UPDATE == 0:  #episodes\n",
    "        print('episode {}, target net updated'.format(episode))\n",
    "        agent.update_target_net\n",
    "    \n",
    "    \n",
    "    # Reset environment at start of episode\n",
    "    seed = torch.randint(250,(1,)).item()\n",
    "    _, _, _ = environment.setRandomSeed(seed)\n",
    "    _, _, _ = environment.reset(environment.training_condition)\n",
    "    \n",
    "    # Create an empty frame stack and fill it with frames\n",
    "    frame_stack = utils.FrameStack(stack_size=STACK_SIZE)\n",
    "    for _ in range(STACK_SIZE):\n",
    "        _, _, frame_raw = environment.step(0)\n",
    "        frame = process_frame(frame_raw).to(DEVICE) \n",
    "        state = frame_stack.update_with(frame)\n",
    "    \n",
    "    # Episode starts here:\n",
    "    for t in count(): \n",
    "        \n",
    "        # 1. Agent performs a step (based on the current state) and obtains next state\n",
    "        action = agent.select_action(state)\n",
    "        side_steps = side_steps + 1  if action != 0 else 0\n",
    "        end, reward, frame_raw = environment.step(action.item())\n",
    "        agent_died = RESET_UPON_END_SIGNAL[end] or side_steps > RESET_AFTER_NR_SIDESTEPS\n",
    "        frame = process_frame(frame_raw).to(DEVICE)\n",
    "        next_state = frame_stack.update_with(frame) if not agent_died else None\n",
    "        \n",
    "        # 2. Interpret reward signal\n",
    "        if reward > 100:\n",
    "            reward = -(reward -100)\n",
    "        \n",
    "        # 3. Push the transition to replay memory (in the right format & shape)\n",
    "        reward = torch.tensor([reward], device=DEVICE,dtype=torch.float)\n",
    "        action = action.unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    \n",
    "        # 4. optimize model\n",
    "        if len(agent.memory) > REPLAY_START_SIZE:\n",
    "            \n",
    "            state_action_values, expected_state_action_values = agent.forward()\n",
    "            \n",
    "            # Compute Huber loss\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(agent.policy_net.parameters(), 1)\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\" @TODO: validation loop\"\"\"\n",
    "        else:\n",
    "            # Do not count as optimization loop\n",
    "            agent.step_count = 0\n",
    "            \n",
    "        # 5. Store performance and training measures\n",
    "        step_count += 1\n",
    "        episode_reward += reward.item();\n",
    "        if end == 1:\n",
    "            box_collisions += 1\n",
    "        if end == 2:\n",
    "            wall_collisions +=1\n",
    "        if side_steps > RESET_AFTER_NR_SIDESTEPS:    \n",
    "            endless_loops +=1\n",
    "\n",
    "        \n",
    "        \n",
    "        # 6. the episode ends here if agent performed any 'lethal' action (specified in RESET_UPON_END_SIGNAL)\n",
    "        if agent_died:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EXAMPLES = 10\n",
    "\n",
    "i = 0\n",
    "\n",
    "# for state, action, next_state, reward in agent.memory.memory[:EXAMPLES]:\n",
    "for state, action, next_state, reward in bad_choices[:EXAMPLES]:\n",
    "    i+=1\n",
    "    \n",
    "    plt.figure(figsize = (10,10), dpi=200)\n",
    "    img = torch.cat([state[0,t,...] for t in range(STACK_SIZE)],dim=1)\n",
    "    if next_state is not None:\n",
    "        img = torch.cat([img, next_state[0,-1,...]],dim=1)\n",
    "        plt.axvline(x=STACK_SIZE*IMSIZE,color='r')\n",
    "    plt.imshow(img.detach().cpu().numpy())\n",
    "    plt.title('Action: {}, Reward {}'.format(action.item(),reward.item()))\n",
    "    plt.axis('off')\n",
    "    plt.ylabel('frames >')\n",
    "    plt.xlabel('state | next state')\n",
    "    plt.show()\n",
    "    \n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_choices = [(state, action, next_state,reward) for (state, action, next_state,reward) in agent.memory.memory if reward<0 and action ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
